Natural Language Processing (NLP) allows you to understand and extract meaningful information (called entities) out of the messages people send. You can then use these entities to identify intent, automate some of your replies, route the conversation to a human via livechat, and collect audience data.
If you are currently leveraging an NLP API, you have to make an extra call when you receive the user message, which adds latency and complexity (example: async, troubleshooting, etc.). With built-in NLP, entities are automatically detected in every text message that someone sends.
Once Messenger's built-in NLP is enabled for your Facebook Page, it automatically detects meaning and intent in every text message before it is sent to your bot. The message will be relayed to your bot as usual, along with any entities detected in the body. See Handling a Message With Entities.
Date and time are automatically localized based on the locale sent in the user's profile.
For example, if someone sends the message, “tomorrow at 2pm” or “2 days before Xmas,” you will get the actual timestamp with the message.
By default, built-in NLP supports the following languages. For other languages, check Customizing NLP via Wit.ai.
Built-in NLP also supports the following advanced settings that let you further customize the nlp object included in messages webhook events. To enable/disable these settings, click the 'Advanced Settings` button:
Verbose mode: Returns extra information like the position of the detected entity in the query.
You must append the nlp_enabled parameter which will either enable or disable NLP for that Page.
Once built-in NLP is enabled, you will see an nlp key in the request sent to your message webhook.
For example, the message, "bye, see you tomorrow at 4pm" would include the following entities:
For each message, the Messenger Platform will return a mapping of the entities that were captured alongside their structured data. The key pieces of information here are the confidence and the value for each entity.
confidence is a value between 0 and 1 that indicates the probability the parser thinks its recognition is correct.
value is the parser output. For example, 2pm can be converted to an ISO string you can use in your bot, like "2017-05-10T14:00:00.000-07:00".
You can learn more about the JSON structure of all the entities in the Wit.ai docs
In your messages webhook, you can update the logic used to respond to messages by taking advantage of Default NLP. For example, if you have a handleMessage() function in your webhook that responds to each message received, you can use the greetings entity to send an appropriate response:
Replicate this logic for other entities, and you will be on your way to using built-in NLP!
Language identification enables you to handle multiple languages using the same bot and Page. Language identification is automatically enabled with built-in NLP. The nlp key in the request sent to to your
message webhook will return the top detected locales for the message. For example, the message “bonjour les amis” would include:
You can assign a different Wit.ai app (using either a Default Model for the supported languages, or a Custom one) for each language you want your bot to support. When Messenger Platform receives a message, it will first find the top detected language, and use the associated Wit.ai app for extracting the entities. If there isn't a Wit.ai app linked to the top detected language, the default language model will be used.
You can customize Messenger's built-in NLP to detect additional entities in English, as well as entities in the 132 languages supported by Wit.ai.
Add your Wit Server access token. You can find your access token in the Wit App settings.
You can also update your NLP settings programmatically with a POST request to the Graph API:
Optional. Specifies whether verbose mode if enabled, which returns extra information like the position of the detected entity in the query.
Optional. The maximum number of trait entities and detected locales to return, in descending order of confidence. Minimum 1. Maximum 8. Defaults to 1.
Optional. Specifies the language support mapping to use. Accepts a JSON object of valid ISO 639-1 language code mapped to object in the form of {"model": Enum, ?"token": string}. Model is either "default" or "custom", and "token" is required when "custom" is specified. Example: {"en": {"model": "default"}, "zh: {"model": "custom", "token": $TOKEN}}. Overrides the previous value..
The built-in NLP integration with the Page inbox lets you create a Wit.ai app automatically and bootstrap it with past conversations from your Page directly from the Facebook app console settings. These samples that are compiled into your Wit.ai app are based on real conversations your bot has had with your users.
Random samples from your past conversations in the Page inbox will be compiled, and will show in a newly created Wit app. The samples will be available for tagging in your Wit.ai app immediately.
While natural language processing isn't a new science, the technology is rapidly advancing thanks to an increased interest in human-to-machine communications, plus an availability of big data, powerful computing and enhanced algorithms. 
As a human, you may speak and write in English, Spanish or Chinese. But a computer's native language – known as machine code or machine language – is largely incomprehensible to most people. At your device's lowest levels, communication occurs not with words but through millions of zeros and ones that produce logical actions. 
Indeed, programmers used punch cards to communicate with the first computers 70 years ago. This manual and arduous process was understood by a relatively small number of people. Now you can say, “Alexa, I like this song,” and a device playing music in your home will lower the volume and reply, “OK. Rating saved,” in a humanlike voice. Then it adapts its algorithm to play that song – and others like it – the next time you listen to that music station. 
Let's take a closer look at that interaction. Your device activated when it heard you speak, understood the unspoken intent in the comment, executed an action and provided feedback in a well-formed English sentence, all in the space of about five seconds. The complete interaction was made possible by NLP, along with other AI elements such as machine learning and deep learning. 
Royal Bank of Scotland uses text analytics, an NLP technique, to extract important trends from customer feedback in many forms. The company analyzes data from emails, surveys and call center conversations to identify the root cause of customer dissatisfaction and implement improvements. Watch the video to learn more about analytics transforming customer relationships.

Natural language processing helps computers communicate with humans in their own language and scales other language-related tasks. For example, NLP makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important. 
Today's machines can analyze more language-based data than humans, without fatigue and in a consistent, unbiased way. Considering the staggering amount of unstructured data that's generated every day, from medical records to social media, automation will be critical to fully analyze text and speech data efficiently.
Human language is astoundingly complex and diverse. We express ourselves in infinite ways, both verbally and in writing. Not only are there hundreds of languages and dialects, but within each language is a unique set of grammar and syntax rules, terms and slang. When we write, we often misspell or abbreviate words, or omit punctuation. When we speak, we have regional accents, and we mumble, stutter and borrow terms from other languages. 
While supervised and unsupervised learning, and specifically deep learning, are now widely used for modeling human language, there's also a need for syntactic and semantic understanding and domain expertise that are not necessarily present in these machine learning approaches. NLP is important because it helps resolve ambiguity in language and adds useful numeric structure to the data for many downstream applications, such as speech recognition or text analytics. 
How are organizations around the world using artificial intelligence and NLP? What are the adoption rates and future plans for these technologies? What are the budgets and deployment plans? And what business problems are being solved with NLP algorithms? Find out in this report from TDWI.
Dignity Health uses NLP and other advanced algorithms to monitor electronic medical records for indications of sepsis. If the probability of a patient having sepsis is high, the system sends an alarm to the primary nurse or physician.
Text analytics is a type of natural language processing that turns text into data for analysis. Learn how organizations in banking, health care and life sciences, manufacturing and government are using text analytics to drive better customer experiences, reduce fraud and improve society.
Natural language processing includes many different techniques for interpreting human language, ranging from statistical and machine learning methods to rules-based and algorithmic approaches. We need a broad array of approaches because the text- and voice-based data varies widely, as do the practical applications. 
Basic NLP tasks include tokenization and parsing, lemmatization/stemming, part-of-speech tagging, language detection and identification of semantic relationships. If you ever diagramed sentences in grade school, you've done these tasks manually before. 
In general terms, NLP tasks break down language into shorter, elemental pieces, try to understand relationships between the pieces and explore how the pieces work together to create meaning.
Content categorization. A linguistic-based document summary, including search and indexing, content alerts and duplication detection.
Topic discovery and modeling. Accurately capture the meaning and themes in text collections, and apply advanced analytics to text, like optimization and forecasting.
Sentiment analysis. Identifying the mood or subjective opinions within large amounts of text, including average sentiment and opinion mining. 

In all these cases, the overarching goal is to take raw language input and use linguistics and algorithms to transform or enrich the text in such a way that it delivers greater value. 
How can you find answers in large volumes of textual data? By combining machine learning with natural language processing and text analytics. Find out how your unstructured data can be analyzed to identify issues, evaluate sentiment, detect emerging trends and spot hidden opportunities.

Natural language processing goes hand in hand with text analytics, which counts, groups and categorizes words to extract structure and meaning from large volumes of content. Text analytics is used to explore textual content and derive new variables from raw text that may be visualized, filtered, or used as inputs to predictive models or other statistical methods.
Investigative discovery. Identify patterns and clues in emails or written reports to help detect and solve crimes.
Subject-matter expertise. Classify content into meaningful topics so you can take action and discover trends.
There are many common and practical applications of NLP in our everyday lives. Beyond conversing with virtual assistants like Alexa or Siri, here are a few more examples: 
Have you ever looked at the emails in your spam folder and noticed similarities in the subject lines? You're seeing Bayesian spam filtering, a statistical NLP technique that compares the words in spam to valid emails to identify junk mail.
Have you ever missed a phone call and read the automatic transcript of the voicemail in your email inbox or smartphone app? That's speech-to-text conversion, an NLP capability.
Have you ever navigated a website by using its built-in search bar, or by selecting suggested topic, entity or category tags? Then you've used NLP methods for search, topic modeling, entity extraction and content categorization.
A subfield of NLP called natural language understanding (NLU) has begun to rise in popularity because of its potential in cognitive and AI applications. NLU goes beyond the structural understanding of language to interpret intent, resolve context and word ambiguity, and even generate well-formed human language on its own. NLU algorithms must tackle the extremely complex problem of semantic interpretation – that is, understanding the intended meaning of spoken or written language, with all the subtleties, context and inferences that we humans are able to comprehend.
The evolution of NLP toward NLU has a lot of important implications for businesses and consumers alike. Imagine the power of an algorithm that can understand the meaning and nuance of human language in many contexts, from medicine to law to the classroom. As the volumes of unstructured information continue to grow exponentially, we will benefit from computers' tireless ability to help us make sense of it all. 






IoT in healthcare: Unlocking true, value-based careGiven the potential of IoT – and the challenges of already overburdened healthcare systems around the world – we can't afford not to integrate IoT in healthcare.







Big data in government: How data and analytics power public programsBig data generated by government and private sources coupled with analytics has become a crucial component for a lot of public-sector work. Why? Because using analytics can improve outcomes of public programs.







Machine learning, data science and AI meet IoTIn this video, Kirk Borne and Michele Null discuss the intersection of machine learning, AI and data science with IoT data and analytics.







Artificial intelligence, machine learning, deep learning and moreArtificial intelligence, machine learning and deep learning are set to change the way we live and work. How do they relate and how are they changing our world?







How do free-to-play video games earn big profits?How does the industry leader of free-to-play, massively multiplayer online games scale its customer intelligence and analytics efforts to thousands of models and terabytes of data a day? With industrialized modeling from SAS.







5 machine learning mistakes and how to avoid themMachine learning is not magic. It presents many of the same challenges as other analytics methods. Learn how to overcome those challenges and incorporate this technique into your analytics strategy.







Can data sharing help cure cancer?Clinical trials can bring new drugs – and new hope – to the market for cancer patients. Now, a new data sharing platform for clinical trial data brings even more hope.







Analytic simulations: Using big data to protect the tiniest patientsAnalytic models help researchers discover the best way to care for babies in the NICU, saving lives (and millions of dollars) in the process.

Short for natural language processing, NLP is a branch of artificial intelligence that deals with analyzing, understanding and generating the languages that humans use naturally in order to interface with computers in both written and spoken contexts using natural human languages instead of computer languages.
One of the challenges inherent in natural language processing is teaching computers to understand the way humans learn and use language. Take, for example, the sentence "Baby swallows fly." This simple sentence has multiple meanings, depending on whether the word "swallows" or the word "fly" is used as the verb, which also determines whether "baby" is used as a noun or an adjective. In the course of human communication, the meaning of the sentence depends on both the context in which it was communicated and each person's understanding of the ambiguity in human languages. This sentence poses problems for software that must first be programmed to understand context and linguistic structures.
Stay up to date on the latest developments in Internet terminology with a free newsletter from Webopedia. Join to subscribe now.
From A3 to ZZZ this guide lists 1,500 text message and online chat abbreviations to help you translate and understand today's texting lingo.        Read More »
The following facts and statistics capture the changing landscape of cloud computing and how service providers and customers are keeping up with...        Read More »
Java is a high-level programming language. This guide describes the basics of Java, providing an overview of syntax, variables, data types and...        Read More »
This second Study Guide describes the basics of Java, providing an overview of operators, modifiers and control Structures.        Read More »
Networking fundamentals teaches the building blocks of modern network design. Learn different types of networks, concepts, architecture and...        Read More »
Natural Language Processing (NLP) applications have become ubiquitous these days. I seem to stumble across websites and applications regularly that are leveraging NLP in one form or another. In short, this is a wonderful time to be involved in the NLP domain.
This rapid increase in NLP adoption has happened largely thanks to the concept of transfer learning enabled through pretrained models. Transfer learning, in the context of NLP, is essentially the ability to train a model on one dataset and then adapt that model to perform different NLP functions on a different dataset.
This breakthrough has made things incredibly easy and simple for everyone, especially folks who don't have the time or resources to build NLP models from scratch. It's perfect for beginners as well who want to learn or transition into NLP.
The author(s) has already put in the effort to design a benchmark model for you! Instead of building a model from scratch to solve a similar NLP problem, we can use that pretrained model on our own NLP dataset
A bit of fine-tuning will be required but it saves us a ton of time and computational resources
In this article, I have showcased the top pretrained models you can use to start your NLP journey and replicate the state-of-the-art research in this field. You can check out my article on the top pretrained models in Computer Vision here.
If you are a beginner in NLP, I recommend taking our popular course – 'NLP using Python'.
Multi-purpose models are the talk of the NLP world. These models power the NLP applications we are excited about –  machine translation, question answering systems, chatbots, sentiment analysis, etc. A core component of these multi-purpose NLP models is the concept of language modelling.
In simple terms, the aim of a language model is to predict the next word or character in a sequence. We'll understand this as we look at each model here.
If you're a NLP enthusiast, you're going to love this section. Now, let's dive into 5 state-of-the-art multi-purpose NLP model frameworks. I have provided links to the research paper and pretrained models for each model. Go ahead and explore them!
ULMFiT was proposed and designed by fast.ai's Jeremy Howard and DeepMind's Sebastian Ruder. You could say that ULMFiT was the release that got the transfer learning party started last year.
As we have covered in this article, ULMFiT achieves state-of-the-art results using novel NLP techniques. This method involves fine-tuning a pretrained language model, trained on the Wikitext 103 dataset, to a new dataset in such a manner that it does not forget what it previously learned.
ULMFiT outperforms numerous state-of-the-art on text classification tasks. What I liked about ULMFiT is that it needs very few examples to produce these impressive results. Makes it easier for folks like you and me to understand and implement it on our machines!
In case you were wondering, ULMFiT stands for Universal Language Model Fine-Tuning. The word 'Universal' is quite apt here – the framework can be applied to almost any NLP task.
The Transformer architecture is at the core of almost all the recent major developments in NLP. It was introduced in 2017 by Google. Back then, recurrent neural networks (RNN) were being used for language tasks, like machine translation and question answering systems.
This Transformer architecture outperformed both RNNs and CNNs (convolutional neural networks). The computational resources required to train models were reduced as well. A win-win for everyone in NLP. Check out the below comparison:
As per Google, Transformer “applies a self-attention mechanism which directly models relationships between all words in a sentence, regardless of their respective position”. It does so using a fixed-sized context (aka the previous words). Too complex to get? Let's take an example to simplify this.
“She found the shells on the bank of the river.” The model needs to understand that “bank” here refers to the shore and not a financial institution. Transformer understands this in a single step. I encourage you to read the full paper I have linked below to gain an understanding of how this works. It will blow your mind.
Google released an improved version of Transformer last year called Universal Transformer. There's an even newer and more intuitive version, called Transformer-XL, which we will cover below.
The BERT framework has been making waves ever since Google published their results, and then open sourced the code behind it. We can debate whether this marks “a new era in NLP“, but there's not a shred of doubt that BERT is a very useful framework that generalizes well to a variety of NLP tasks.
BERT, short for Bidirectional Encoder Representations, considers the context from both sides (left and right) of a word. All previous efforts considered one side of a word at a time – either the left or the right. This bidirectionality helps the model gain a much better understanding of the context in which the word(s) was used. Additionally, BERT is designed to do multi-task learning, that is, it can perform different NLP tasks simultaneously.
BERT is the first unsupervised, deeply bidirectional system for pretraining NLP models. It was trained using only a plain text corpus.
At the time of its release, BERT was producing state-of-the-art results on 11 Natural Language Processing (NLP) tasks. Quite a monumental feat! You can train your own NLP model (such as a question-answering system) using BERT in just a few hours (on a single GPU).
This release by Google could potentially be a very important one in the long-term for NLP. This concept could become a bit tricky if you're a beginner so I encourage you to read it a few times to grasp it. I have also provided multiple resources below this section to help you get started with Transformer-XL.
Picture this – you're halfway through a book and suddenly a word or sentence comes up that was referred to at the start of the book. Now, you or I can recall what it was. But a machine, understandably, struggles to model long-term dependency.
One way to do this, as we saw above, is by using Transformers. But they are implemented with a fixed-length context. In other words, there's not much flexibility to go around if you use this approach.
Transformer-XL bridges that gap really well. Developed by the Google AI team, it is a novel NLP architecture that helps machines understand context beyond that fixed-length limitation. Transformer-XL is up to 1800 times faster than a typical Transformer.
Transformer-XL, as you might have predicted by now, achieves new state-of-the-art results on various language modeling benchmarks/datasets. Here's a small table taken from their page illustrating this:
The Transformer-XL GitHub repository, linked above and mentioned below, contains the code in both PyTorch and TensorFlow.
Now, this is a pretty controversial entry. A few people might argue that the release of GPT-2 was a marketing stunt by OpenAI. I certainly understand where they're coming from. However, I believe it's important to still at least try out the code OpenAI has released.
First, some context for those who are not aware what I'm talking about. OpenAI penned a blog post (link below) in February where they claimed to have designed a NLP model, called GPT-2, that was so good that they couldn't afford to release the full version for fear of malicious use. That certainly got the community's attention.
GPT-2 was trained to predict the next occurring word in 40GB of internet text data. This framework is also a transformer-based model trained on a dataset of 8 million web pages. The results they have published on their site are nothing short of astounding. The model is able to weave an entirely legible story based on a few sentences we input. Check out this example:
The developers have released a much smaller version of GPT-2 for researchers and engineers to test. The original model has 1.5 billion parameters – the open source sample model has 117 million.
Most of the machine learning and deep learning algorithms we use are incapable of working directly with strings and plain text. These techniques require us to convert text data into numbers before they can perform any task (such as regression or classification).
So in simple terms, word embeddings are the text blocks that are converted into numbers for performing NLP tasks. A word bmbedding format generally tries to map a word using a dictionary to a vector.
You can get a much more in-depth explanation of word embeddings, its different types, and how to use them on a dataset in the below article. If you are not familiar with the concept, I consider this guide a must-read:
In this section, we'll look at two state-of-the-art word embeddings for NLP. I have also provided tutorial links so you can get a practical understanding of each topic.
No, this ELMo isn't the (admittedly awesome) character from Sesame Street. But this ELMo, short for Embeddings from Language Models, is pretty useful in the context of building NLP models.
ELMo is a novel way of representing words in vectors and embeddings. These ELMo word embeddings help us achieve state-of-the-art results on multiple NLP tasks, as shown below:
Let's take a moment to understand how ELMo works. Recall what we discussed about bidirectional language models earlier. Taking a cue from this article, “ELMo word vectors are computed on top of a two-layer bidirectional language model (biLM). This biLM model has two layers stacked together. Each layer has 2 passes — forward pass and backward pass:
ELMo word representations consider the full input sentence for calculating the word embeddings. So, the term “read” would have different ELMo vectors under different context. A far cry from the older word embeddings when the same vector would be assigned to the word “read” regardless of the context in which it was used.
Flair is not exactly a word embedding, but a combination of word embeddings. We can call Flair more of a NLP library that combines embeddings such as GloVe, BERT, ELMo, etc. The good folks at Zalando Research developed and open-sourced Flair.
'Flair Embedding' is the signature embedding that comes packaged within the Flair library. It is powered by contextual string embeddings. You should go through this article to understand the core components that power Flair.
What I especially like about Flair is that it supports multiple languages. So many NLP releases are stuck doing English tasks. We need to expand beyond this if NLP is to gain traction globally!
Speaking of expanding NLP beyond the English language, here's a library that is already setting benchmarks. The authors claim that StanfordNLP supports over 53 languages – that certainly got our attention!
Our team was among the first to work with the library and publish the results on a real-world dataset. We played around with it and found that StanfordNLP truly does open up a lot of possibilities of applying NLP techniques on non-English languages. like Hindi, Chinese and Japanese.
StanfordNLP is a collection of pretrained state-of-the-art NLP models. These models aren't just lab tested – they were used by the authors in the CoNLL 2017 and 2018 competitions. All the pretrained NLP models packaged in StanfordNLP are built on PyTorch and can be trained and evaluated on your own annotated data.
Full neural network pipeline for performing text analytics, including:

Tokenization
Multi-word token (MWT) expansion
Lemmatization
Parts-of-speech (POS) and morphological feature tagging
Dependency Parsing


This is by no means an exhaustive list of pretrained NLP models. There are a lot more available and you can check out a few of them on this site.
I would love to hear your thoughts on this list. Have you used any of these pretrained models before? Or you have perhaps explored other options? Let me know in the comments section below – I will be happy to check them out and add them to this list.


24 Ultimate Data Science Projects To Boost Your Knowledge and Skills (& can be accessed freely)


Natural language processing (NLP) helps computers understand and interpret human language by breaking down the elemental pieces of speech.
Natural Language Processing, or NLP,  focuses on interactions between computers and human languages. NLP is a field that brings together computer science, artificial intelligence, and linguistics.
Computers are great at handling structured data such as database tables and spreadsheets.  But human language is incredibly diverse and complex, and often far from tightly-structured. Human language spans across hundreds of languages and dialects, with large sets of grammar rules, syntaxes, terms, and slang.
Homonyms, or words that are spelled and sound the same but carry different meanings, create an interesting NLP problem. "Paris Hilton listens to Paris Hilton at the Paris Hilton" is a sentence that native English speakers don't have too much trouble parsing but creates a complicated NLP problem. When does "Paris" refer to a person, and when does it signify a hotel's location in France?
Natural Language Processing allows computers to communicate with humans in their own language by pulling meaningful data from loosely-structured text or speech. NLP helps scale language-related tasks. This is what makes it possible for computers to read text (or hear speech), interpret that text or speech, and determine what to do with the information.
NLP helps to resolve ambiguity in language by adding numeric structure to large datasets. This structure makes speech recognition and text analytics possible.
The field of NLP has grown rapidly in the last decade. Thanks to advancements in the field of natural language processing and technologies built on it, someone can now say to a device in their home, "Hey Google, play Never Gonna Give You Up" and hear their favorite song played back to them.
At its core, NLP helps computers understand and even interact with human speech. Natural language processing relies on techniques ranging from statistical machine learning methods to various algorithmic approaches.
Due to the natural variances in human speech, voice and text-based data quality vary widely. This broad spectrum of approaches leveraged by NLP allows for a wide range of applications.
You may have dabbled in natural language processing yourself if you ever had to diagram a sentence in school. Tagging various elements of speech, detecting which language is being spoken or written, or identifying semantic relationships between words are all core NLP tasks.
Content categorization, topic modeling, sentiment analysis, speech-to-text transcription, and text-to-speech conversion all leverage these core NLP tasks.
Semantic analysis is closely related to NLP and helps form the backbone of how computers process human language.
At its core, semantic analysis helps connect a specific word or set of words to contextual meaning. This is what allows humans to understand our "Paris Hilton" example above. A computer needs to leverage semantic analysis to determine if "Paris" refers to a human's name, an artist's catalog, or a city in France.
Twilio provides speech recognition, which leverages Natural Language Processing to convert speech to text in real-time during a phone call. This allows your system to analyze meaning and intent.
Twilio has also built out a robust Natural Language Understanding (NLU) engine that powers Understand. This engine turns text from phone calls and messages into structured data that your virtual assistant or other applications can leverage.
With Twilio Understand, you can pivot from DTMF-powered phone tree menus to conversational interactions between your system and your users. Learn more about Understand in this blog post.
Natural Language Processing is a vast and complex field of study. You can learn more about NLP  through these resources:
Find a wide variety of resources that can help get you started in NLP with this curated list of content.
Enroll in an online course that teaches Natural Language Processing, like this one from Coursera.
This course provides an overview of natural language processing (NLP) on modern Intel® architecture. Topics include:
The course is structured around eight weeks of lectures and exercises. Each week requires three hours to complete.
The history of natural language processes and how it is used in the industry today
How computers encode pieces of text into a document-term matrix and what the bag of words assumption is
A typical machine learning workflow for two different machine learning approaches to classify emails as either spam or not spam
This class teaches an algorithm for natural language understanding and topic modeling. Learn more about:
How to use the latent Dirichlet allocation algorithm to extract topics from the document-term matrices
This class continues to teach how to model and extract topics in text. Learn more about:
Continuing with the topic of machine learning, this class teaches more about applying neural networks. Topics include:﻿﻿
Natural Language Processing is the technology used to aid computers to understand the human's natural language.
Leand Romaf, an experienced software engineer who is passionate at teaching people how artificial intelligence systems work, says that “in recent years, there have been significant breakthroughs in empowering computers to understand language just as we do.”
This article will give a simple introduction to Natural Language Processing and how it can be achieved.
Natural Language Processing, usually shortened as NLP, is a branch of artificial intelligence that deals with the interaction between computers and humans using the natural language.
The ultimate objective of NLP is to read, decipher, understand, and make sense of the human languages in a manner that is valuable.
In fact, a typical interaction between humans and machines using Natural Language Processing could go as follows:
Word Processors such as Microsoft Word and Grammarly that employ NLP to check grammatical accuracy of texts.
Interactive Voice Response (IVR) applications used in call centers to respond to certain users' requests.
Natural Language processing is considered a difficult problem in computer science. It's the nature of the human language that makes NLP difficult.
The rules that dictate the passing of information using natural languages are not easy for computers to understand.
Some of these rules can be high-leveled and abstract; for example, when someone uses a sarcastic remark to pass information.
On the other hand, some of these rules can be low-levelled; for example, using the character “s” to signify the plurality of items.
Comprehensively understanding the human language requires understanding both the words and how the concepts are connected to deliver the intended message.
While humans can easily master a language, the ambiguity and imprecise characteristics of the natural languages are what make NLP difficult for machines to implement.
NLP entails applying algorithms to identify and extract the natural language rules such that the unstructured language data is converted into a form that computers can understand.
When the text has been provided, the computer will utilize algorithms to extract meaning associated with every sentence and collect the essential data from them.
Sometimes, the computer may fail to understand the meaning of a sentence well, leading to obscure results.
For example, a humorous incident occurred in the 1950s during the translation of some words between the English and the Russian languages.
Here is the result when the sentence was translated to Russian and back to English:
Syntactic analysis and semantic analysis are the main techniques used to complete Natural Language Processing tasks.
Syntax refers to the arrangement of words in a sentence such that they make grammatical sense.
In NLP, syntactic analysis is used to assess how the natural language aligns with the grammatical rules.
Computer algorithms are used to apply grammatical rules to a group of words and derive meaning from them.
Lemmatization: It entails reducing the various inflected forms of a word into a single form for easy analysis.
Semantics refers to the meaning that is conveyed by a text. Semantic analysis is one of the difficult aspects of Natural Language Processing that has not been fully resolved yet.
It involves applying computer algorithms to understand the meaning and interpretation of words and how sentences are structured.
Named entity recognition (NER): It involves determining the parts of a text that can be identified and categorized into preset groups. Examples of such groups include names of people and names of places.
Natural language generation: It involves using databases to derive semantic intentions and convert them into human language.
As more research is being carried in this field, we expect to see more breakthroughs that will make machines smarter at recognizing and understanding the human language.
Over the last two years, the Natural Language Processing community has witnessed an acceleration in progress on a wide range of different tasks and applications. 🚀 This progress was enabled by a shift of paradigm in the way we classically build an NLP system: for a long time, we used pre-trained word embeddings such as word2vec or GloVe to initialize the first layer of a neural network, followed by a task-specific architecture that is trained in a supervised way using a single dataset.
Recently, several works demonstrated that we can learn hierarchical contextualized representations on web-scale datasets 📖 leveraging unsupervised (or self-supervised) signals such as language modeling and transfer this pre-training to downstream tasks (Transfer Learning). Excitingly, this shift led to significant advances on a wide range of downstream applications ranging from Question Answering, to Natural Language Inference through Syntactic Parsing…
A few weeks ago, a friend of mine decided to dive in into NLP. He already has a background in Machine Learning and Deep Learning so he genuinely asked me: “Which papers can I read to catch up with the latest trends in modern NLP?”. 👩‍🎓👨‍🎓
That's a really good question, especially when you factor in that NLP conferences (and ML conferences in general) receive an exponentially growing number of submissions: +80% NAACL 2019 VS 2018, +90% ACL 2019 VS 2018, …
I compiled this list of papers and resources 📚 for him, and I thought it would be great to share it with the community since I believe it can be useful for a lot of people.
Disclaimer: this list is not intended to be exhaustive, nor to cover every single topic in NLP (for instance, there is nothing on Semantic Parsing, Adversarial Learning, Reinforcement Learning applied to NLP,…). It is rather a pick of the most recent impactful works in the past few years/months (as of May 2019), mostly influenced by what I read.
Generally speaking, a good way to start is to read introductive or summary blog posts with a high-level view that gives you enough context ✋ before actually spending time reading a paper (for instance this post or this one).
Deep contextualized word representations (NAACL 2018)Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer
Language Models are Unsupervised Multitask LearnersAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (NAACL 2019)Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
Cloze-driven Pretraining of Self-attention Networks (arXiv 2019)Alexei Baevski, Sergey Edunov, Yinhan Liu, Luke Zettlemoyer, Michael Auli
Unified Language Model Pre-training for Natural Language Understanding and Generation (arXiv 2019)Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, Hsiao-Wuen Hon
MASS: Masked Sequence to Sequence Pre-training for Language Generation (ICML 2019)Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, Tie-Yan Liu
What you can cram into a single vector: Probing sentence embeddings for linguistic properties (ACL 2018)Alexis Conneau, German Kruszewski, Guillaume Lample, Loïc Barrault, Marco Baroni
GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding (ICLR 2019)Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. BowmanandSuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems (arXiv 2019)Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, Samuel R. Bowman
Linguistic Knowledge and Transferability of Contextual Representations (NAACL 2019)Nelson F. Liu, Matt Gardner, Yonatan Belinkov, Matthew E. Peters, Noah A. Smith
To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks (arXiv 2019)Matthew Peters, Sebastian Ruder, Noah A. Smith
A Persona-Based Neural Conversation Model (ACL 2016)Jiwei Li, Michel Galley, Chris Brockett, Georgios P. Spithourakis, Jianfeng Gao, Bill Dolan
A Simple, Fast Diverse Decoding Algorithm for Neural Generation (arXiv 2017)Jiwei Li, Will Monroe, Dan Jurafsky
TransferTransfo: A Transfer Learning Approach for Neural Network Based Conversational Agents (NeurIPS 2018 CAI Workshop)Thomas Wolf, Victor Sanh, Julien Chaumond, Clement DelangueDisclaimer: I am an author on this publication.Step by step explanation blog post
Wizard of Wikipedia: Knowledge-Powered Conversational agents (ICLR 2019)Emily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, Jason Weston
Learning to Speak and Act in a Fantasy Text Adventure Game (arXiv 2019)Jack Urbanek, Angela Fan, Siddharth Karamcheti, Saachi Jain, Samuel Humeau, Emily Dinan, Tim Rocktäschel, Douwe Kiela, Arthur Szlam, Jason Weston
Get To The Point: Summarization with Pointer-Generator Networks (ACL 2017)Abigail See, Peter J. Liu, Christopher D. Manning
Supervised Learning of Universal Sentence Representations from Natural Language Inference Data (EMNLP 2017)Alexis Conneau, Douwe Kiela, Holger Schwenk, Loic Barrault, Antoine Bordes
StarSpace: Embed All The Things! (AAAI 2018)Ledell Wu, Adam Fisch, Sumit Chopra, Keith Adams, Antoine Bordes, Jason Weston
The Natural Language Decathlon: Multitask Learning as Question Answering (arXiv 2018)Bryan McCann, Nitish Shirish Keskar, Caiming Xiong, Richard Socher
Character-Level Language Modeling with Deeper Self-Attention (arXiv 2018)Rami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, Llion Jones
Linguistically-Informed Self-Attention for Semantic Role Labeling (EMNLP 2018)Emma Strubell, Patrick Verga, Daniel Andor, David Weiss, Andrew McCallum
Phrase-Based & Neural Unsupervised Machine Translation (EMNLP 2018)Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, Marc'Aurelio Ranzato
Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning (ICLR 2018)Sandeep Subramanian, Adam Trischler, Yoshua Bengio, Christopher J Pal
Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (arXiv 2019)Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V. Le, Ruslan Salakhutdinov
An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models (NAACL 2019)Alexandra Chronopoulou, Christos Baziotis, Alexandros Potamianos
… for older papers, the number of citations is generally a reasonable proxy when choosing what to read.
As a good rule of thumb, you should read papers that you find interesting and spark joy in you! 🤷‍♂️🌟
There are plenty of amazing resources available you can use that are not necessarily papers. Here are a few:
CS224n: Natural Language Processing with Deep Learning with Chris Manning and Abigail See at Standford
That's it for the pointers! Reading a few of these resources should already give you a good sense of the latest trends in contemporary NLP and hopefully, help you build your own NLP system! 🎮
One last thing that I did not talk about much in this post, but that I find extremely important (and sometimes neglected) is that reading is good, implementing is better! 👩‍💻 You'll often learn so much more by supplementing your reading with diving into the (sometimes) attached code or trying to implement some of it yourself. Practical resources include the amazing blog posts and courses from fast.ai or our 🤗 open-source repositories.
What about you? What are the works that had the most impact on you? Tell us in the comments! ⌨️
As always, if you liked this post, give us a few 👏 to let us know and share the news around you!
Many thanks to Lysandre Debut, Clément Delangue, Thibault Févry, Peter Martigny, Anthony Moi and Thomas Wolf for their comments and feedback.
Natural language processing (NLP) is a subfield of computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.

Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.

3 Major evaluations and tasks

3.1 Syntax
3.2 Semantics
3.3 Discourse
3.4 Speech
3.5 Dialogue


The history of natural language processing generally started in the 1950s, although work can be found from earlier periods.
In 1950, Alan Turing published an article titled "Intelligence" which proposed what is now called the Turing test as a criterion of intelligence.

The Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2]  However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced.  Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.

Some notably successful natural language processing systems developed in the 1960s were SHRDLU, a natural language system working in restricted "blocks worlds" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966.  Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the "patient" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to "My head hurts" with "Why do you say your head hurts?".

During the 1970s, many programmers began to write "conceptual ontologies", which structured real-world information into computer-understandable data.  Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981).  During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.

Up to the 1980s,most natural language processing systems were based on complex sets of hand-written rules.  Starting in the late 1980s, however, there was a revolution in natural language processing with the introduction of machine learning algorithms for language processing.  This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules.  However, part-of-speech tagging introduced the use of hidden Markov models to natural language processing, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models.  Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.

Many of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed.  These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government.  However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.

Recent research has increasingly focused on unsupervised and semi-supervised learning algorithms.  Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data.  Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data.  However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results if the algorithm used has a low enough time complexity to be practical.

In the 2010s, representation learning and deep neural network-style machine learning methods became widespread in natural language processing, due in part to a flurry of results showing that such techniques[4][5] can achieve state-of-the-art results in many natural language tasks, for example in language modeling,[6]
parsing,[7][8] and many others. Popular techniques include the use of word embeddings to capture semantic properties of words, and an increase in end-to-end learning of a higher-level task (e.g., question answering) instead of relying on a pipeline of separate intermediate tasks (e.g., part-of-speech tagging and dependency parsing). In some areas, this shift has entailed substantial changes in how NLP systems are designed, such that deep neural network-based approaches may be viewed as a new paradigm distinct from statistical natural language processing. For instance, the term neural machine translation (NMT) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations, obviating the need for intermediate steps such as word alignment and language modeling that were used in statistical machine translation (SMT).

In the early days, many language-processing systems were designed by hand-coding a set of rules,[9][10], e.g. by writing grammars or devising heuristic rules for stemming.
However, this is rarely robust to natural language variation. 

Since the so-called "statistical revolution"[11][12]
in the late 1980s and mid 1990s, much natural language processing research has relied heavily on machine learning.

The machine-learning paradigm calls instead for using statistical inference to automatically learn such rules through the analysis of large corpora of typical real-world examples (a corpus (plural, "corpora") is a set of documents, possibly with human or computer annotations).

Many different classes of machine-learning algorithms have been applied to natural-language-processing tasks. These algorithms take as input a large set of "features" that are generated from the input data. Some of the earliest-used algorithms, such as decision trees, produced systems of hard if-then rules similar to the systems of hand-written rules that were then common. Increasingly, however, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to each input feature. Such models have the advantage that they can express the relative certainty of many different possible answers rather than only one, producing more reliable results when such a model is included as a component of a larger system.

The learning procedures used during machine learning automatically focus on the most common cases, whereas when writing rules by hand it is often not at all obvious where the effort should be directed.
Automatic learning procedures can make use of statistical-inference algorithms to produce models that are robust to unfamiliar input (e.g. containing words or structures that have not been seen before) and to erroneous input (e.g. with misspelled words or words accidentally omitted). Generally, handling such input gracefully with hand-written rules—or, more generally, creating systems of hand-written rules that make soft decisions—is extremely difficult, error-prone and time-consuming.
Systems based on automatically learning the rules can be made more accurate simply by supplying more input data. However, systems based on hand-written rules can only be made more accurate by increasing the complexity of the rules, which is a much more difficult task.  In particular, there is a limit to the complexity of systems based on hand-crafted rules, beyond which the systems become more and more unmanageable.  However, creating more data to input to machine-learning systems simply requires a corresponding increase in the number of man-hours worked, generally without significant increases in the complexity of the annotation process.
The following is a list of some of the most commonly researched tasks in natural language processing.  Note that some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.

Though natural language processing tasks are closely intertwined, they are frequently subdivided into categories for convenience. A coarse division is given below.

The first published work by an artificial intelligence was published in 2018, 1 the Road, marketed as a novel, contains sixty million words.

The first major study was conducted in 2013, on the occasion of the anniversary of the Association for Computational Linguistics (ACL) with a workshop called: "Rediscovering 50 Years of Discoveries in Natural Language Processing [19].
The same year, started the NLP4NLP project with the aim of discovering which terms are introduced along the years, with details concerning the authors and the conferences involved 
[20].
Then the project was extended to other directions while covering 34 conferences in speech and NLP. A full synthesis of the NLP4NLP project has been published in 2019 under the form of a double publication in Frontiers in Research Metrics and Analytics 
covering 50 years of publications[21]
[22]
.

Bates, M (1995). "Models of natural language understanding". Proceedings of the National Academy of Sciences of the United States of America. 92 (22): 9977–9982. doi:10.1073/pnas.92.22.9977. PMC 40721. PMID 7479812.
Steven Bird, Ewan Klein, and Edward Loper (2009). Natural Language Processing with Python. O'Reilly Media. ISBN 978-0-596-51649-9.
Daniel Jurafsky and James H. Martin (2008). Speech and Language Processing, 2nd edition. Pearson Prentice Hall. ISBN 978-0-13-187321-6.
Mohamed Zakaria Kurdi (2016). Natural Language Processing and Computational Linguistics: speech, morphology, and syntax, Volume 1. ISTE-Wiley. ISBN 978-1848218482.
Mohamed Zakaria Kurdi (2017). Natural Language Processing and Computational Linguistics: semantics, discourse, and applications, Volume 2. ISTE-Wiley. ISBN 978-1848219212.
Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze (2008). Introduction to Information Retrieval. Cambridge University Press. ISBN 978-0-521-86571-5. Official html and pdf versions available without charge.
Christopher D. Manning and Hinrich Schütze (1999). Foundations of Statistical Natural Language Processing. The MIT Press. ISBN 978-0-262-13360-9.
David M. W. Powers and Christopher C. R. Turk (1989). Machine Learning of Natural Language. Springer-Verlag. ISBN 978-0-387-19557-5.
The essence of Natural Language Processing lies in making computers understand the natural language. That's not an easy task though. Computers can understand the structured form of data like spreadsheets and the tables in the database, but human languages, texts, and voices form an unstructured category of data, and it gets difficult for the computer to understand it, and there arises the need for Natural Language Processing.
There's a lot of natural language data out there in various forms and it would get very easy if computers can understand and process that data. We can train the models in accordance with expected output in different ways. Humans have been writing for thousands of years, there are a lot of literature pieces available, and it would be great if we make computers understand that. But the task is never going to be easy. There are various challenges floating out there like understanding the correct meaning of the sentence, correct Named-Entity Recognition(NER), correct prediction of various parts of speech, coreference resolution(the most challenging thing in my opinion).
Computers can't truly understand the human language. If we feed enough data and train a model properly, it can distinguish and try categorizing various parts of speech(noun, verb, adjective, supporter, etc…) based on previously fed data and experiences. If it encounters a new word it tried making the nearest guess which can be embarrassingly wrong few times.
It's very difficult for a computer to extract the exact meaning from a sentence. For example – The boy radiated fire like vibes. The boy had a very motivating personality or he actually radiated fire? As you see over here, parsing English with a computer is going to be complicated.
There are various stages involved in training a model. Solving a complex problem in Machine Learning means building a pipeline. In simple terms, it means breaking a complex problem into a number of small problems, making models for each of them and then integrating these models. A similar thing is done in NLP. We can break down the process of understanding English for a model into a number of small pieces.
It would be really great if a computer could understand that San Pedro is an island in Belize district in Central America with a population of 16, 444 and it is the second largest town in Belize. But to make the computer understand this, we need to teach computer very basic concepts of written language.
So let's start by creating an NLP pipeline. It has various steps which will give us the desired output(maybe not in a few rare cases) at the end.
Step #1: Sentence Segmentation
Breaking the piece of text in various sentences.
Input : San Pedro is a town on the southern part of the island of Ambergris Caye in the Belize District of the nation of Belize, in Central America. According to 2015 mid-year estimates, the town has a population of about 16, 444. It is the second-largest town in the Belize District and largest in the Belize Rural South constituency.
Output : San Pedro is a town on the southern part of the island of Ambergris Caye in the 2.Belize District of the nation of Belize, in Central America.
According to 2015 mid-year estimates, the town has a population of about 16, 444.
It is the second-largest town in the Belize District and largest in the Belize Rural South constituency.
For coding a sentence segmentation model, we can consider splitting a sentence when it encounters any punctuation mark. But modern NLP pipelines have techniques to split even if the document isn't formatted properly.

Step #2: Word Tokenization
Breaking the sentence into individual words called as tokens. We can tokenize them whenever we encounter a space, we can train a model in that way. Even punctuations are considered as individual tokens as they have some meaning. 
Input : San Pedro is a town on the southern part of the island of Ambergris Caye in the Belize District of the nation of Belize, in Central America. According to 2015 mid-year estimates, the town has a population of about 16, 444. It is the second-largest town in the Belize District and largest in the Belize Rural South constituency.
Output : 'San Pedro', ' is', 'a', 'town' and so.

Step #3: Predicting Parts of Speech for each token
Predicting whether the word is a noun, verb, adjective, adverb, pronoun, etc. This will help to understand what the sentence is talking about. This can be achieved by feeding the tokens( and the words around it) to a pre-trained part-of-speech classification model. This model was fed a lot of English words with various parts of speech tagged to them so that it classifies the similar words it encounters in future in various parts of speech. Again, the models don't really understand the 'sense' of the words, it just classifies them on the basis of its previous experience. It's pure statistics.
The process will look like this: 






Input : Part of speech classification model 
Output : Town - common noun
         Is - verb 
         The - determiner

And similarly, it will classify various tokens.
Step #4: Lemmatization
Feeding the model with the root word.
For example –
There's a Buffalo grazing in the field. 
There are Buffaloes grazing in the field. 
Here, both Buffalo and Buffaloes mean the same. But, the computer can confuse it as two different terms as it doesn't know anything. So we have to teach the computer that both terms mean the same. We have to tell a computer that both sentences are talking about the same concept. So we need to find out the most basic form or root form or lemma of the word and feed it to the model accordingly.
In a similar fashion, we can use it for verbs too. 'Play' and 'Playing' should be considered as same.
Step #5: Identifying stop words
There are various words in the English language that are used very frequently like 'a', 'and', 'the' etc. These words make a lot of noise while doing statistical analysis. We can take these words out. Some NLP pipelines will categorize these words as stop words, they will be filtered out while doing some statistical analysis. Definitely, they are needed to understand the dependency between various tokens to get the exact sense of the sentence. The list of stop words varies and depends on what kind of output are you expecting.
Step 6.1: Dependency Parsing
This means finding out the relationship between the words in the sentence and how they are related to each other. We create a parse tree in dependency parsing, with root as the main verb in the sentence. If we talk about the first sentence in our example, then 'is' is the main verb and it will be the root of the parse tree. We can construct a parse tree of every sentence with one root word(main verb) associated with it. We can also identify the kind of relationship that exists between the two words. In our example, 'San Pedro' is the subject and 'island' is the attribute. Thus, the relationship between 'San Pedro' and 'is', and 'island' and 'is' can be established.
Just like we trained a Machine Learning model to identify various parts of speech, we can train a model to identify the dependency between words by feeding many words. It's a complex task though. In 2016, Google released a new dependency parser Parsey McParseface which used a deep learning approach.
Step 6.2: Finding Noun Phrases
We can group the words that represent the same idea. For example – It is the second-largest town in the Belize District and largest in the Belize Rural South constituency. Here, tokens 'second', 'largest' and 'town' can be grouped together as they together represent the same thing 'Belize'. We can use the output of dependency parsing to combine such words. Whether to do this step or not completely depends on the end goal, but it's always quick to do this if we don't want much information about which words are adjective, rather focus on other important details.
Step #7: Named Entity Recognition(NER)
San Pedro is a town on the southern part of the island of Ambergris Caye in the 2. Belize District of the nation of Belize, in Central America.
Here, the NER maps the words with the real world places. The places that actually exist in the physical world. We can automatically extract the real world places present in the document using NLP.
If the above sentence is the input, NER will map it like this way: 
San Pedro - Geographic Entity
Ambergris Caye - Geographic Entity
Belize - Geographic Entity
Central America - Geographic Entity
NER systems look for how a word is placed in a sentence and make use of other statistical models to identify what kind of word actually it is. For example – 'Washington' can be a geographical location as well as the last name of any person. A good NER system can identify this.
Kinds of objects that a typical NER system can tag: 
People's names. 
Company names. 
Geographical locations
Product names. 
Date and time. 
Amount of money. 
Events.

Step #8: Coreference Resolution:
San Pedro is a town on the southern part of the island of Ambergris Caye in the Belize District of the nation of Belize, in Central America. According to 2015 mid-year estimates, the town has a population of about 16, 444. It is the second-largest town in the Belize District and largest in the Belize Rural South constituency.
Here, we know that 'it' in the sentence 6 stands for San Pedro, but for a computer, it isn't possible to understand that both the tokens are same because it treats both the sentences as two different things while it's processing them. Pronouns are used with a high frequency in English literature and it becomes difficult for a computer to understand that both things are same.
If you like GeeksforGeeks and would like to contribute, you can also write an article using contribute.geeksforgeeks.org or mail your article to contribute@geeksforgeeks.org. See your article appearing on the GeeksforGeeks main page and help other Geeks.
Please Improve this article if you find anything incorrect by clicking on  the "Improve Article" button below.
We advance the state of the art in natural language technologies and build systems that learn to understand and use language in context.
Our team comprises multiple research groups working on a range of Language projects. We collaborate closely with teams across Google, leveraging efficient algorithms, neural networks, and graphical and probabilistic models to help guide product development and direction. In doing so, the Language team enables natural and assistive communication with users, finds answers to user questions, analyzes app store reviews for developers, and more.
Our researchers are experts in traditional natural language processing and machine learning, and combine methodological research with applied science. All of our Language engineers are equally involved in long-term research efforts and driving immediate applications of our technology. Our systems also benefit greatly from Google linguists, who provide valuable labelled data and assist in enabling internationalization.
Recent research interests of the Language team include syntax, discourse, conversation, multilingual modeling, sentiment analysis, question answering, summarization, and generally building better learners using labeled and unlabeled data, state-of-the-art modeling, and indirect supervision.
Current state-of-the-art semantic role labeling (SRL) uses a deep neural network with no explicit linguistic features. However, prior work has shown that gold syntax trees can dramatically improve SRL decoding, suggesting the possibility of increased accuracy from explicit modeling of syntax. In this work, we present linguistically-informed self-attention (LISA): a neural network model that combines multi-head self-attention with multi-task learning across dependency parsing, part-of-speech tagging, predicate detection and SRL. Unlike previous models which require significant pre-processing to prepare linguistic features, LISA can incorporate...
We address the problem of fine-grained multilingual language identification: providing a language code for every token in a sentence, including codemixed text containing multiple languages.  Such text is  increasingly prevalent online, in documents, social media, and message boards. In this paper, we show that a feed-forward network with a simple globally constrained decoder can accurately and rapidly label both codemixed and monolingual text in 100 languages and 100 language pairs. This model outperforms previously published multilingual approaches in terms of both accuracy and speed, yielding an 800x speed-up and a 19.2%% averaged absolute ...
We present models for encoding sentences into embedding vectors that specifically target transfer learning to other NLP tasks. The models are efficient and result in accurate performance on diverse transfer tasks. Two variants of the encoding models allow for trade-offs between accuracy and compute resources. For both variants, we investigate and report the relationship between model complexity, resource consumption, the availability of transfer task training data, and task performance. Comparisons are made with baselines that use word level transfer learning via pretrained word embeddings as well as baselines do not use any transfer learning...

Daniel Cer,  Yinfei Yang,  Sheng-yi Kong,  Nan Hua,  Nicole Lyn Untalan Limtiaco,  Rhomni  St. John,  Noah Constant,  Mario Guajardo-CÃ©spedes,  Steve Yuan,  Chris Tar,  Yun-hsuan Sung,  Brian Strope,  Ray Kurzweil 
The rise of neural networks, and particularly recurrent neural networks, has produced significant advances in part-of-speech tagging accuracy. One characteristic common among these models is the presence of rich initial word encodings. These encodings typically are composed of a recurrent character-based representation with learned and pre-trained word embeddings. However, these encodings do not consider a context wider than a single word and it is only through subsequent recurrent layers that word or sub-word information interacts. In this paper, we investigate models that use recurrent neural networks with sentence-level context for initial...
To help spur development in open-domain question answering, we have created the Natural Questions (NQ) corpus, along with a challenge website based on this data.
We released TensorFlow code and models for BERT, a novel pre-training technique which achieves state-of-the-art results on 11 natural language processing tasks. 
Active Question Answering (ActiveQA) is a TensorFlow package that investigates using reinforcement learning to train artificial agents for question answering.
We released a new dataset consisting of ~3.3 million image/caption pairs and an image captioning challenge for the ML community to train and evaluate their own models on the Conceptual Captions test bed.
Based on our examination of the use of Smart Reply in Inbox and our ideas about how humans learn and use language, we have created a new version of Smart Reply for Gmail.
Learn language representations that capture meaning at various levels of granularity, shared and resuable across domains.
Use state-of-the-art Machine Learning techniques and large-scale infrastructure to break language barriers and offer human quality translations across many languages to make it possible to easily explore multilingual world.
Learn end-to-end models for real world question answering that require complex reasoning about concepts, entities, relations, and causality in the world.
Learn document representations from geometric features and spatial relations, multi-modal content features, syntactic, semantic and pragmatic signals.
Advance next generation dialogue systems in human-machine and multi-human-machine interactions to achieve natural user interactions and enrich conversations between human users.
Learn to summarize single and multiple documents into cohesive and concise summaries that accurately represent the documents.
Produce natural and fluent output for spoken and written text for different domains and styles.
Learn to extend models to support new languages easily, and deal effectively with mixed-code language.
Understand visual inputs (image & video) and express that understanding using fluent natural language (phrases, sentences, paragraphs).
Most of Googleâs users interact with us through language. Working on the Language team means you get to play a critical role in helping our systems understand what users want.
The Language team provides opportunities to work on ambitious research projects and to share successes along the way with products and the academic community.

                                                    Take your understanding of unstructured data to a whole new level with a full suite of advanced text analytics features to extract entities, relationships, keywords, semantic roles and more.
                                                

                                                   Apply the knowledge of unique entities and relations in your industry or organization to your data.
                                                
An NLU item is based on the number of data units enriched and the number of enrichment features applied. A data unit is 10,000 characters or less. For example: extracting Entities and Sentiment from 15,000 characters of text is (2 Data Units * 2 Enrichment Features) = 4 NLU Items.

                                                    Watson Premium plans offer a higher level of security and isolation to help customers with sensitive data requirements.
                                                

                                              LegalMation developes a first-of-its-kind AI platform to automate routine litigation tasks, using IBM Watson. LegalMation uses Watson Discovery offerings to draft early phase response documents, which helped legal teams save time, drive down costs and shift strategic focus. See how Legalmation assembles a team of subject matter experts (SMEs) to use IBM Watson Knowledge Studio and IBM Watson Natural Language Understanding to create a domain-specific model focused on legal terminology and concepts.
                                            

                                              Influential leverages AI and IBM Watson to enable influencers to amplify their marketing messages through social media. Augmented intelligence through IBM Watson allows influencers to target campaigns towards strategic demographics. See how Influential uses IBM Watson Natural Language Understanding, IBM Watson Personality Insights and IBM Watson Tone Analyzer application programming interfaces (APIs) on the IBM Cloud Platform to improve social campaign performance.
                                            

                                              Max Kelsen uses IBM Watson to build an insight engine for powering an AI platform that could provide insights into customer experience. The company collaborates with the local government to understand and query large amounts of private and public data. See how Max Kelsen uses IBM Watson Discovery, IBM Watson Natural Language Understanding, and IBM Watson Knowledge Studio to deliver insights on citizens interests.
                                            
Natural Language Processing (NLP) refers to AI method of communicating with an intelligent systems using a natural language such as English.
Processing of Natural Language is required when you want an intelligent system like robot to perform as per your instructions, when you want to hear decision from a dialogue based clinical expert system, etc.
The field of NLP involves making computers to perform useful tasks with the natural languages humans use. The input and output of an NLP system can be −
It is the process of producing meaningful phrases and sentences in the form of natural language from some internal representation.
Sentence planning − It includes choosing required words, forming meaningful phrases, setting tone of the sentence.
For example, “He lifted the beetle with red cap.” − Did he use cap to lift the beetle or he lifted a beetle that had red cap?
Referential ambiguity − Referring to something using pronouns. For example, Rima went to Gauri. She said, “I am tired.” − Exactly who is tired?
Syntax − It refers to arranging words to make a sentence. It also involves determining the structural role of words in the sentence and in phrases.
Semantics − It is concerned with the meaning of words and how to combine words into meaningful phrases and sentences.
Pragmatics − It deals with using and understanding sentences in different situations and how the interpretation of the sentence is affected.
Discourse − It deals with how the immediately preceding sentence can affect the interpretation of the next sentence.
Lexical Analysis − It involves identifying and analyzing the structure of words. Lexicon of a language means the collection of words and phrases in a language. Lexical analysis is dividing the whole chunk of txt into paragraphs, sentences, and words.
Syntactic Analysis (Parsing) − It involves analysis of words in the sentence for grammar and arranging words in a manner that shows the relationship among the words. The sentence such as “The school goes to boy” is rejected by English syntactic analyzer.
They are not highly precise. For example, “The grains peck the bird”, is a syntactically correct according to parser, but even if it makes no sense, parser takes it as a correct sentence.
To bring out high precision, multiple sets of grammar need to be prepared. It may require a completely different sets of rules for parsing singular and plural variations, passive sentences, etc., which can lead to creation of huge set of rules that are unmanageable.
It is inefficient, as the search process has to be repeated if an error occurs.

      We use cookies to provide and improve our services. By using our site, you consent to our Cookies Policy.
      Accept
Learn more

Welcome to the best Natural Language Processing course on the internet! This course is designed to be your complete online resource for learning how to use Natural Language Processing with the Python programming language.
In the course we will cover everything you need to learn in order to become a world class practitioner of NLP with Python. 
We'll start off with the basics, learning how to open and work with text and PDF files with Python, as well as learning how to use regular expressions to search for custom patterns inside of text files.
Afterwards we will begin with the basics of Natural Language Processing, utilizing the Natural Language Toolkit library for Python, as well as the state of the art Spacy library for ultra fast tokenization, parsing, entity recognition, and lemmatization of text.
We'll understand fundamental NLP concepts such as stemming, lemmatization, stop words, phrase matching, tokenization and more!
Next we will cover Part-of-Speech tagging, where your Python scripts will be able to automatically assign words in text to their appropriate part of speech, such as nouns, verbs and adjectives, an essential part of building intelligent language systems.
We'll also learn about named entity recognition, allowing your code to automatically understand concepts like money, time, companies, products, and more simply by supplying the text information.
Through state of the art visualization libraries we will be able view these relationships in real time.
Then we will move on to understanding machine learning with Scikit-Learn to conduct text classification, such as automatically building machine learning systems that can determine positive versus negative movie reviews, or spam versus legitimate email messages.
We will expand this knowledge to more complex unsupervised learning methods for natural language processing, such as topic modelling, where our machine learning models will detect topics and major concepts from raw text files.
This course even covers advanced topics, such as sentiment analysis of text with the NLTK library, and creating semantic word vectors with the Word2Vec algorithm.
Included in this course is an entire section devoted to state of the art advanced topics, such as using deep learning to build out our own chat bots!
Not only do you get fantastic technical content with this course, but you will also get access to both our course related Question and Answer forums, as well as our live student chat channel, so you can team up with other students for projects, or get help on the course content from myself and the course teaching assistants. 
All of this comes with a 30 day money back garuantee, so you can try the course risk free.

Natural Language Processing (or: Natural Language Programming, in short: NLP) is a technology that enables computers and people to communicate with each other at eye level. NLP combines linguistic findings with the latest methods of computer science and artificial intelligence.

In order for Natural Language Processing to work, speech recognition must firstly be developed. NLP is seen as a promising technology in the field of Human Computer Interaction (HCI) for the control of devices or web applications. For example, the work of chatbots or digital language assistants is based on this principle.

The development of NLP dates back to the 1950s, when the scientist Alan Turing published an article entitled "Computing Machinery and Intelligence", in which he presented a method for measuring artificial intelligence. The so-called "Turing Test" is still in existence today.

As early as 1954, researchers had already succeeded in translating sixty sentences into English using a machine. Euphoric about this start, many computer scientists thought that machine translation was just a matter of time. However, the first systems for statistically-based machine translation were further developed in the 1980s. In the meantime, certain approaches have been found to translate information from the "real" world into computer language.

A major evolutionary step was made in the late 1980s. Machine Learning became popular around this time. Together with the ever-increasing computing power of computers, NLP algorithms could now be used. One of the pioneers in this field was and still is the linguist Noam Chomsky. The software company IBM also ensured the increasing development of Natural Language Processing.

Today, NLP-based computer programs can no longer only access manually collected data sets, but are also able to analyze text corpora such as websites or spoken language directly.

NLP is based on the basic idea that any form of language, spoken or written, must first be recognized. However, language is a very complex system of characters. It is not only a single word that is important, but its connection with other words, entire sentences or facts.

What people naturally learn from birth, computers have to achieve with the help of algorithms. While the human being can fall back on his life experience, the computer must be able to fall back on artificially generated experiences. The challenge for the machine processing of natural language is therefore not so much to produce language as to understand it.

Modern NLP is based on algorithms, which in turn are based on statistical machine learning. The unique thing about this is that computers are not only able to learn on the basis of previously learned dilemmas, but can also independently identify problems and solve new problem areas on the basis of large documentation. Computers do not learn to find a solution for every problem, but they learn general patterns that help them to solve individual problems. This makes NLP a precursor for artificial intelligence.

Google. When it was first established, the project was seen as laughable. Today, the program is able to translate many different texts and even the spoken word fairly fluently.

Google's established "Rank Brain" also uses the Natural Language Processing method to deliver matching results to unprecedented search queries. The "interpreting" of input is supplemented by artificial intelligence.

Speech recognition as a central task area at NLP depends on many different factors. Here, the most important ones are summarized briefly.

 Automated summary: The programs must be able to automatically reduce large texts to the essentials.
 Relationship between words within a sentence: Here, NLP is required to recognize which sentence elements are related to each other.
Example: I'm sitting in the back seat of my car. In this case, the program must recognize that the rear seat belongs to the car.

 Discourse Analysis: NLP software must be able to recognize the register of a text (raised, colloquial) in the register. The program must also recognize the type of text (purchase note, invoice, request).
 Machine translation: NLP-based programs must be able to translate the human language into another human language and master grammar, semantics and other linguistic sub-areas.
 Morphological segmentation: Under this term, the decomposition of a word into its individual components is described.
 NER (Named Entity Recognition): An NLP program must recognize whether a text contains proper names for places, persons or organizations and must be able to assign them. For the text output, the program must therefore also know for Western languages whether the words in question are capitalized.
 Optical character recognition (OCR): This is an image recognition that can convert images into text, as some scanners can do today.
 Recognition of word meanings: Sound can "book" both the action of a ticket purchase and the majority of the tree "beech".
NLP is an important building block in the development of artificial intelligence. Language plays a central role in the creation of autonomous computers. The Natural Language Processing approach thus forms the important interface between human beings and computers.

Today, these techniques are used in the translation of documents, in the processing of documents, but also in call centres. In the meantime, there are also programs that can create texts on their own.

Services such as Skype should soon be able to translate telephone conversation live. Today, users can already "talk" to chatbots of selected providers on Skype to book tickets or to start simple queries. Google also wants to turn its Translator into a live translator. At the same time, the technology is used by numerous digital assistants from major Internet companies, such as Amazon Echo, Windows Cortana or Siri from Apple.

Welcome to TNW's beginner's guide to AI. This (currently) four part feature should provide you with a very basic understanding of what AI is, what it can do, and how it works. The guide contains articles on (in order published) neural networks, computer vision, natural language processing, and algorithms. It's not necessary to read them all, but doing so may better help your understanding of the topics covered. 
When computers process text or audio from humans, they're just looking at data. You can whisper “I love you” or drop an F-bomb, the machines just see 1's and 0's. In order for AI to understand what you're saying, turn those words into an action, and then output something you can understand, they rely on something called natural language processing (NLP), which is exactly what it sounds like.
You no longer need a degree in computer science to interact with a machine learning program. In fact, rather than learn how to talk to the machines, millions of us are paying for the privilege of teaching computers from Amazon, Apple, Google, Microsoft, and countless other companies how to talk to us
Many years ago, before Siri was launched in 2011, people and computers couldn't speak the same language. Believe it or not, you could shout “play some Skynyrd” at a computer and it wouldn't so much as acknowledge your command.
Back in those days the computers weren't being jerks, they simply didn't have the benefit of deep learning networks to drive their natural language processing. Things have changed a lot since 2010 – and even more since the ideas behind virtual assistants like Siri were first developed in the 1940s.
Now your average iPhone has more processing power than the IBM mainframes that many machine learning developers used over half a century ago. And the algorithms, don't get me started! They're far more robust and capable than we would have thought possible just a few decades ago.
These remarkable technology breakthroughs spurred the deep learning revolution we're currently enjoying and with it, natural language processing evolved from a time-consuming process where rules are handwritten by humans, to an unsupervised learning trick where computers understand how we communicate better than most of us do.
The way it works can vary depending on the system. There's no simple way to describe all of the modules that go into building an AI as complex as, say, Google's Assistant. But the idea behind it isn't all that complicated.
Basically researchers create a neural network that ingests as much data as possible, we're talking about thousands or millions of files full of information. This data could be audio files that correspond to annotated text, or the other way around. The idea is that if you say a word, the computer will learn to interpret and respond to you based on what it's trained on.
There are, of course, variations on the above theme – and many NLP functions are far less intensive. You could develop a simple algorithm that parses huge files full of text for specific words or phrases, for example.
Google's Search used to just parse text, now it tries to interpret what you ask it
Virtual assistants have become so good at NLP you can play Skyrim on a smart speaker
And the list goes on and on. A large portion of the AI-powered gadgets being marketed to the general public are NLP products.
If you'd like to learn more about natural language processing check out the following resources:
Here's a Coursera course on machine learning that's lead by Google Brain co-founder Andrew Ng.
Here's a Udacity course that's more specific to NLP, like the above it is not free.
If free is more your speed you can learn about NLP the old fashioned way: check out Hackster.IO's projects here.
And don't forget to visit our artificial intelligence section for all the latest machine learning news and analysis.



4


				iOS 13 will show you where apps have tracked your location, on a map			



Ivan Mehta








                Sit back and let the hottest tech news come to you by the magic of electronic mail.
            

                Got two minutes to spare? We'd love to know a bit more about our readers.
Start!

Our team of IT marketing professionals and digital enthusiasts are passionate about semantic technology and cognitive computing and how it will transform our world. 
We'll keep you posted on the latest Expert System products, solutions and services, and share the most interesting information on semantics, cognitive computing and AI from around the web, and from our rich library of white papers, customer case studies and more.
This Website uses third party analytical and profiling cookies to offer services in line with the preferences you reveal while using the Website. If you want to learn more, or want to opt-out of cookies, click here Privacy Policy. If you continue to browse or select any links or options on the Website, you will be deemed as consenting to the use of these cookies.
Natural-language understanding (NLU) or natural-language interpretation (NLI)[1] is a subtopic  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.[2]

There is considerable commercial interest in the field because of its application to automated reasoning,[3] machine translation,[4] question answering,[5] news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.

NLU is the post-processing of text, after the use of NLP algorithms (identifying parts-of-speech, etc.), that utilizes context from recognition devices (automatic speech recognition [ASR], vision recognition, last conversation, misrecognized words from ASR, personalized profiles, microphone proximity etc.), in all of its forms, to discern meaning of fragmented and run-on sentences to execute an intent from typically voice commands.  NLU has an ontology around the particular product vertical that is used to figure out the probability of some intent.  An NLU has a defined list of known intents that derives the message payload from designated contextual information recognition sources.  The NLU will provide back multiple message outputs to separate services (software) or resources (hardware) from a single derived intent (response to voice command initiator with visual sentence (shown or spoken) and transformed voice command message too different output messages to be consumed for M2M communications and actions).[]

The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural-language understanding by a computer.[6][7][8][9][10]  Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.

A year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.[11]

In 1969 Roger Schank at Stanford University introduced the conceptual dependency theory for natural-language understanding.[12] This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.

In 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input.[13] Instead of phrase structure rules ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called "generalized ATNs" continued to be used for a number of years.

In 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field.[14][15] Winograd continued to be a major influence in the field with the publication of his book Language as a Cognitive Process.[16] At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.

In the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, e.g., in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, e.g., Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp.[17][18] In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnart.[19]

The third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, it is debated how much "understanding" such systems demonstrate, e.g. according to John Searle, Watson did not even understand the questions.[20]

John Ball, cognitive scientist and inventor of Patom Theory supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional natural language processing. "To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence – just like a 3-year-old does without guesswork" Patom Theory

The umbrella term "natural-language understanding" can be applied to a diverse set of computer applications, ranging from small, relatively simple tasks such as short commands issued to robots, to highly complex endeavors such as the full comprehension of newspaper articles or poetry passages. Many real world applications fall between the two extremes, for instance text classification for the automatic analysis of emails and their routing to a suitable department in a corporation does not require in depth understanding of the text,[21] but needs to deal with a much larger vocabulary and more diverse syntax than the management of simple queries to database tables with fixed schemata.

Throughout the years various attempts at processing natural language or English-like sentences presented to computers have taken place at varying degrees of complexity. Some attempts have not resulted in systems with deep understanding, but have helped overall system usability. For example, Wayne Ratliff originally developed the Vulcan program with an English-like syntax to mimic the English speaking computer in Star Trek. Vulcan later became the dBase system whose easy-to-use syntax effectively launched the personal computer database industry.[22][23] Systems with an easy to use or English like syntax are, however, quite distinct from systems that use a rich lexicon and include an internal representation (often as first order logic) of the semantics of natural language sentences.

Hence the breadth and depth of "understanding" aimed at by a system determine both the complexity of the system (and the implied challenges) and the types of applications it can deal with.  The "breadth" of a system is measured by the sizes of its vocabulary and grammar.  The "depth" is measured by the degree to which its understanding approximates that of a fluent native speaker.  At the narrowest and shallowest, English-like command interpreters require minimal complexity, but have a small range of applications.  Narrow but deep systems explore and model mechanisms of understanding,[24] but they still have limited application.  Systems that attempt to understand the contents of a document such as a news release beyond simple keyword matching and to judge its suitability for a user are broader and require significant complexity,[25] but they are still somewhat shallow.  Systems that are both very broad and very deep are beyond the current state of the art.

Regardless of the approach used, most natural-language-understanding systems share some common components. The system needs a lexicon of the language and a parser and grammar rules to break sentences into an internal representation. The construction of a rich lexicon with a suitable ontology requires significant effort, e.g., the Wordnet lexicon required many person-years of effort.[26]

The system also needs a semantic theory to guide the comprehension. The interpretation capabilities of a language-understanding system depend on the semantic theory it uses. Competing semantic theories of language have specific trade-offs in their suitability as the basis of computer-automated semantic interpretation.[27] These range from naive semantics or stochastic semantic analysis to the use of pragmatics to derive meaning from context.[28][29][30] Semantic parsers convert natural-language texts into formal meaning representations.[31]

Advanced applications of natural-language understanding also attempt to incorporate logical inference within their framework. This is generally achieved by mapping the derived meaning into a set of assertions in predicate logic, then using logical deduction to arrive at conclusions. Therefore, systems based on functional languages such as Lisp need to include a subsystem to represent logical assertions, while logic-oriented systems such as those using the language Prolog generally rely on an extension of the built-in logical representation framework.[32][33]

The management of context in natural-language understanding can present special challenges. A large variety of examples and counter examples have resulted in multiple approaches to the formal modeling of context, each with specific strengths and weaknesses.[34][35]

Natural Language Processing, or NLP for short, is broadly defined as the automatic manipulation of natural language, like speech and text, by software.
The study of natural language processing has been around for more than 50 years and grew out of the field of linguistics with the rise of computers.
In this post, you will discover what natural language processing is and why it is so important.
Where the field of NLP came from and how it is defined by modern practitioners.
We may speak to each other, as a species, more than we write. It may even be easier to learn to speak than to write.
Given the importance of this type of data, we must have methods to understand and reason about natural language, just like we do for other types of data.
It is hard from the standpoint of the child, who must spend many years acquiring a language … it is hard for the adult language learner, it is hard for the scientist who attempts to model the relevant phenomena, and it is hard for the engineer who attempts to build systems that deal with natural language input or output. These tasks are so hard that Turing could rightly make fluent conversation in natural language the centerpiece of his test for intelligence.
Human language is highly ambiguous … It is also ever changing and evolving. People are great at producing language and understanding language, and are capable of expressing, perceiving, and interpreting very elaborate and nuanced meanings. At the same time, while we humans are great users of language, we are also very poor at formally understanding and describing the rules that govern language.
Classical linguistics involved devising and evaluating rules of language. Great progress was made on formal methods for syntax and semantics, but for the most part, the interesting problems in natural language understanding resist clean mathematical formalisms.
Broadly, a linguist is anyone who studies language, but perhaps more colloquially, a self-defining linguist may be more focused on being out in the field.
Mathematics is the tool of science. Mathematicians working on natural language may refer to their study as mathematical linguistics, focusing exclusively on the use of discrete mathematical formalisms and theory for natural language (e.g. formal languages and automata theory).
Computational linguistics is the modern study of linguistics using the tools of computer science. Yesterday's linguistics may be today's computational linguist as the use of computational tools and thinking has overtaken most fields of study.
Computational linguistics is the study of computer systems for understanding and generating natural language. … One natural function for computational linguistics would be the testing of grammars proposed by theoretical linguists.
Large data and fast computers mean that new and different things can be discovered from large datasets of text by writing and running software.
In the 1990s, statistical methods and statistical machine learning began to and eventually replaced the classical top-down rule-based approaches to language, primarily because of their better results, speed, and robustness. The statistical approach to studying natural language now dominates the field; it may define the field.
Data-Drive methods for natural language processing have now become so popular that they must be considered mainstream approaches to computational linguistics. … A strong contributing factor to this development is undoubtedly the increase amount of available electronically stored data to which these methods can be applied; another factor might be a certain disenchantment with approaches relying exclusively on hand-crafted rules, due to their observed brittleness.
The statistical approach to natural language is not limited to statistics per-se, but also to advanced inference methods like those used in applied machine learning.
… understanding natural language require large amounts of knowledge about morphology, syntax, semantics and pragmatics as well as general knowledge about the world. Acquiring and encoding all of this knowledge is one of the fundamental impediments to developing effective and robust language systems. Like the statistical methods … machine learning methods off the promise of automatic the acquisition of this knowledge from annotated or unannotated language corpora.
Computational linguistics also became known by the name of natural language process, or NLP, to reflect the more engineer-based or empirical approach of the statistical methods.
The statistical dominance of the field also often leads to NLP being described as Statistical Natural Language Processing, perhaps to distance it from the classical computational linguistics methods.
I view computational linguistics as having both a scientific and an engineering side. The engineering side of computational linguistics, often called natural language processing (NLP), is largely concerned with building computational tools that do useful things with language, e.g., machine translation, summarization, question-answering, etc. Like any engineering discipline, natural language processing draws on a variety of different scientific disciplines.
Linguistics is a large topic of study, and, although the statistical approach to NLP has shown great success in some areas, there is still room and great benefit from the classical top-down methods.
Roughly speaking, statistical NLP associates probabilities with the alternatives encountered in the course of analyzing an utterance or a text and accepts the most probable outcome as the correct one. … Not surprisingly, words that name phenomena that are closely related in the world, or our perception of it, frequently occur close to one another so that crisp facts about the world are reflected in somewhat fuzzier facts about texts. There is much room for debate in this view.
As machine learning practitioners interested in working with text data, we are concerned with the tools and methods from the field of Natural Language Processing.
We have seen the path from linguistics to NLP in the previous section. Now, let's take a look at how modern researchers and practitioners define what NLP is all about.
In perhaps one of the more widely textbooks written by top researchers in the field, they refer to the subject as “linguistic science,” permitting discussion of both classical linguistics and modern statistical methods.
The aim of a linguistic science is to be able to characterize and explain the multitude of linguistic observations circling around us, in conversations, writing, and other media. Part of that has to do with the cognitive size of how humans acquire, produce and understand language, part of it has to do with understanding the relationship between linguistic utterances and the world, and part of it has to do with understand the linguistic structures by which language communicates.
They go on to focus on inference through the use of statistical methods in natural language processing.
Statistical NLP aims to do statistical inference for the field of natural language. Statistical inference in general consists of taking some data (generated in accordance with some unknown probability distribution) and then making some inference about this distribution.
In their text on applied natural language processing, the authors and contributors to the popular NLTK Python library for NLP describe the field broadly as using computers to work with natural language data.
We will take Natural Language Processing — or NLP for short –in a wide sense to cover any kind of computer manipulation of natural language. At one extreme, it could be as simple as counting word frequencies to compare different writing styles. At the other extreme, NLP involves “understanding” complete human utterances, at least to the extent of being able to give useful responses to them.
Statistical NLP has turned another corner and is now strongly focused on the use of deep learning neural networks to both perform inference on specific tasks and for developing robust end-to-end systems.
In one of the first textbooks dedicated to this emerging topic, Yoav Goldberg succinctly defines NLP as automatic methods that take natural language as input or produce natural language as output.
Natural language processing (NLP) is a collective term referring to automatic computational processing of human languages. This includes both algorithms that take human-produced text as input, and algorithms that produce natural looking text as outputs.
In this post, you discovered what natural language processing is why it is so important.
Do you have any questions?
Ask your questions in the comments below and I will do my best to answer.
It provides self-study tutorials on topics like:
Bag-of-Words, Word Embedding, Language Models, Caption Generation, Text Translation and much more…
Hi, I'm Jason Brownlee, PhD.

I write tutorials to help developers (like you) get results with machine learning.

 How to Develop LSTM Models for Multi-Step Time Series Forecasting of Household Power Consumption
October 10, 2018



 How to Develop RNN Models for Human Activity Recognition Time Series Classification
September 24, 2018


Natural language processing (NLP) is a method to translate between computer and human languages. It is a method of getting a computer to understandably read a line of text without the computer being fed some sort of clue or calculation. In other words, NLP automates the translation process between computers and humans.
Traditionally, feeding statistics and models have been the method of choice for interpreting phrases. Recent advances in this area include voice recognition software, human language translation, information retrieval and artificial intelligence. There is difficulty in developing human language translation software because language is constantly changing. Natural language processing is also being developed to create human readable text and to translate between one human language and another. The ultimate goal of NLP is to build software that will analyze, understand and generate human languages naturally, enabling communication with a computer as if it were a human.
Bernard Marr is an internationally best-selling author, popular keynote speaker, futurist, and a strategic business & technology advisor to governments and companies...
Natural Language Processing (NLP) is a field of data science and artificial intelligence that studies how computers and languages interact. The goal of NLP is to program a computer to understand human speech as it is spoken.
Current approaches to NLP are based on machine learning — i.e. examining patterns in natural language data, and using these patterns to improve a computer program's language comprehension. Chatbots, smartphone personal assistants, search engines, banking applications, translation software, and many other business applications use natural language processing techniques to parse and understand human speech and written text.
And behind all of that savvy marketing, exceptional customer service, strategic upselling, etc., is one key component making it happen: big data integration.
One common application of NLP is a chatbot. If a user opens an online business chat to troubleshoot or ask a question, a computer responds in a manner that mimics a human. Sometimes the user doesn't even know he or she is chatting with an algorithm.
That chatbot is trained using thousands of conversation logs, i.e. big data. A language processing layer in the computer system accesses a knowledge base (source content) and data storage (interaction history and NLP analytics) to come up with an answer. Big data and the integration of big data with machine learning allow developers to create and train a chatbot.
Natural language processing can also be used to process free form text and analyze the sentiment of a large group of social media users, such as Twitter followers, to determine whether the target group response is negative, positive, or neutral. The process is known as “sentiment analysis” and can easily provide brands and organizations with a broad view of how a target audience responded to an ad, product, news story, etc.
Natural language processing is built on big data, but the technology brings new capabilities and efficiencies to big data as well.
A simple example is log analysis and log mining. One common NLP technique is lexical analysis — the process of identifying and analyzing the structure of words and phrases. In computer sciences, it is better known as parsing or tokenization, and used to convert an array of log data into a uniform structure.
A more nuanced example is the increasing capabilities of natural language processing to glean business intelligence from terabytes of data. Traditionally, it is the job of a small team of experts at an organization to collect, aggregate, and analyze data in order to extract meaningful business insights. But those individuals need to know where to find the data they need, which keywords to use, etc. NLP is increasingly able to recognize patterns and make meaningful connections in data on its own.
Natural language processing deals with phonology (the study of the system of relationships among sounds in language) and morphology (the study of word forms and their relationships), and works by breaking down language into its component pieces.
The first step in NLP is to convert text into data using text analytics, which occurs at three levels:
The next stage involves using NLP and natural language understanding (NLU) to analyze the structure and meaning of the data. A few approaches to NLP analysis are:
Distributional Approach — Uses statistical tactics of machine learning to identify the meaning of a word by how it is used, such as part-of-speech tagging (Is this a noun or verb?) and semantic relatedness (different words that are used in similar ways).
Frame-Based Approach — Uses a canonical presentation of sentences, represented inside the data structure (frame), to identify parts of the sentences that are syntactically different but semantically the same,
Interactive Learning Approach — Uses dynamic, interactive environments where the user teaches the machine how to learn a language, step-by-step.
There are many approaches to natural language analysis — some very complex. Four fundamental, commonly used techniques in NLP analysis are:
Lexical Analysis — Lexical analysis groups streams of letters or sounds from source code into basic units of meaning, called tokens. These tokens are then used by a language compiler to implement computer instructions, such as a chatbot responding to a question.
Syntactic Analysis — Syntactic analysis is the process of analyzing words in a sentence for grammar, using a parsing algorithm, then arranging the words in a way that shows the relationship among them. Parsing algorithms break the words down into smaller parts—strings of natural language symbols—then analyze these strings of symbols to determine if they conform to a set of established grammatical rules.
Semantic Analysis — Semantic analysis involves obtaining the meaning of a sentence, called the logical form, from possible parses of the syntax stage. It involves understanding the relationship between words, such as semantic relatedness — i.e. when different words are used in similar ways.
Pragmatic Analysis — Pragmatic analysis is the process of discovering the meaning of a sentence based on context. It attempts to understand the ways humans produce and comprehend meaning from text or human speech. Pragmatic analysis in NLP would be the task of teaching a computer to understand the meaning of a sentence in different real-life situations.
Software applications using NLP and AI are expected to be a $5.4 billion market by 2025. The possibilities for both big data, and the industries it powers, are almost endless.
The healthcare industry will continue to benefit dramatically from better natural language processing and data integration. Communications will be more efficient, errors will be reduced, and diagnoses will improve when healthcare software can understand and integrate hastily written or typed doctors notes, patients' phone calls and voicemails, and more. In addition, Pharmaceutical companies and medical researchers often have tomes of text data in clinical trial information, patient notes, etc. Analyzing that information will be much more efficient with tools like automatic summarization.
Law enforcement will benefit from a system that can understand and integrate language-turned-data from social media posts, criminal records, and anonymous phone calls and tips.
Legal firms will benefit when pages and pages of legal documents, stenographer notes, testimonies, and/or police reports can be translated to data and easily summarized.
Specific NLP processes like automatic summarization — analyzing a large volume of text data and producing an executive summary — will be a boon to many industries, including some that may not have been considered “big data industries” until now.
And big data processes will, themselves, continue to benefit from improved NLP capabilities. So many data processes are about translating information from humans (language) to computers (data) for processing, and then translating it from computers (data) to humans (language) for analysis and decision making. As natural language processing continues to become more and more savvy, our big data capabilities can only become more and more sophisticated.
NLP uses various analyses (lexical, syntactic, semantic, and pragmatic) to make it possible for computers to read, hear, and analyze language-based data. As a result, technologies such as chatbots are able to mimic human speech, and search engines are able to deliver more accurate results to users' queries.
Talend Studio with machine learning on Spark can be used to teach a computer to understand how humans use natural language. To get started, download Talend Open Studio for Big Data.
For more details on the basics of machine learning and NLP, start with our on-demand webinar, Fundamentals of Machine Learning. It covers what machine learning is, types of machine learning, machine learning algorithms, how Talend enables machine learning with Spark, and more.
 Talend is widely recognized as a leader in data integration and quality tools. Start your first project in minutes!
Don't miss out! Sign up for our newsletter to get all the information you need.
As members explore the many products of LinkedIn, including the feed, homepage, Learning, Recruiter, and Sales Navigator, to name a few, they often experience new exciting features, which, from time to time, may lead to questions around use. To help our members and deliver the assistance they need, we use deep learning-powered Natural Language Processing (NLP) to predict the best answers for help requests. Given our scale, NLP gives us the best shot at providing answers for our 610 million+ members. Every day, the automated system based on NLP solves over one thousand tickets. The questions can range from broad, such as on accounts settings, to specific, like enhancing a member's profile page. To ensure members can get the help they need easily, we offer the Quick Help Widget under the Settings tab, and a dedicated Help Center search.
This blog shares more details about how we leverage NLP to process the questions from our members so they can easily use and enjoy all the features of LinkedIn.
In 2016, we built the first iteration of our Help search system, called Care Search. We did not use NLP at the time to solve this problem. Instead, we created an algorithm that calculated the score of every help article based on title, body, and keywords. This score was then used to rank the best answers to our members' help requests.
Our Care Search was built upon Galene, which is a Search-as-a-Service (SeaS) infrastructure that powers a multitude of search products at LinkedIn. The underlying index is a Lucene index. During the search phase, we scored each hit (help center article) with the BM25F algorithm, which is a per-field based TF-IDF (term frequency-inverse document frequency) algorithm. The idea behind it is that for each article, there is a title, body, and potentially many keywords. Each field should have a different weight on the scoring. For example, for the query of "premium membership," articles with "premium" in the title should have higher relevance scoring than those with "premium" in the body. Thus, when calculating the BM25F score, we give the highest weight to hits in the title, then hits in keywords, and lastly, hits in the body. By using this strategy, we were able to return statistically-valuable results for popular and well-formed queries.
Why the system wasn't meeting our needs It turned out that, in Help searches, members tended to use a variety of expressions for questions on “what,” “how,” or “where.” This made it very different from queries for People Search or Job Search. In many cases, the search expression was different from the terminology used in our articles. As a result, people who searched for "how to deactivate my account" would get served articles titled "create account" on the top, since we did not have "deactivate" in the index. Another example is how a search of "uploaded my CV" would return nothing because our index only contained "resume" and "upload."
To better understand members' queries and to improve the quality of the search result, we developed a NLP workflow: 
Text analysis: A query like "how cancelling my premium accounts immediately" becomes normalized to "cancel premium account." 
Query mapping: Based on the member's query, we will find a popular query like "cancel premium subscription." 
Enter deep learning: We identify the intent of the query using a Convolutional Neural Network (CNN). 
Text analysis Text analyzer is used to parse the text and simplify the text to extract the core information. It is the basis of all the NLP components in our system, both for query processing and index building. It has four steps as shown in Figure 1:
Lemmatization: Find the basic form of each word variation in the context. Examples are "account" from "accounts" and "break" from "broke." 
Stop Word Filter: Filter out common words. In English, there are hundreds of stop words like "a," "my," and "on," to name a few, that have little bearing on relevance or meaning, and thus can safely be removed from the query in order to target the more valuable words. 
Part of Speech (PoS) Filter: Read through text and give each word a PoS based on the context. There are nine parts in English: adjective, adverb, conjunction, determiner, noun, number, preposition, pronoun, and verb. We only capture nouns, verbs, proper nouns, and adjectives, as they together represent the purpose of a text. 
Query mapping Query mapping will convert the simplified query generated from text analysis to a rep query, which is more relevant to the article data. Inspecting members query history, we found that there were some queries with good search results, while others had not not-so-good results. The latter mostly occurred when the query did not match any of the words in the articles. To fill the gap between a member's terminology and a given article's terminology, we've built a representative query mapping to convert the raw query into more representative queries. For example, imagine a set of member queries on closing accounts: "how to unsubscribe linkedin," "leave linkedin," "delete linkedin," and "close account." Among these queries, "close account" gets the best search results, since it matches the title of the target article "Closing Your LinkedIn Account." So, we say "close account" is the rep query for the other three queries. To get the rep query for each raw query requires two parts:
Query grouping: We first calculate the edit distance of raw queries. This is the number of operations (insert/delete/substitute) needed to transform one text into another. For instance, the edit distance between "close account" and "closed accounts" is two. Then, we define the similarity of two queries with the following:
sim = 1 – d / max(q1, q2), where d is the edit distance between two queries, and max(q1, q2) is the maximum number of characters in both queries.
Secondly, Jaccard index is also employed to measure the similarity of two queries at the word-level. It represents the number of overlapping words against total unique words between two queries. For example, there are two shared words between "cancel premium subscription" and "cancel premium membership," but four unique words in total. Thus, the Jaccard index is ½.
Topic mining: Query grouping puts similar queries together. However, for a given query, it does not tell which query in its group is the most relevant. To figure that out, we first utilize the text analyzer to extract all the topics from articles. Then, we use a TF-IDF algorithm to filter out popular topics. For example, when it comes to the article "Merging or Closing Duplicate Accounts on LinkedIn," the extracted highest ranked topics will be "merge connection," "merge duplicate account," "close duplicate account," and "find other account." Then, within each query group, we calculate the rep score:
rep score = max(0.2 * sim(rq, q) + 0.5 * sim(q, title) + 0.3 * sim(q, body))
This is for all queries q that are similar to the raw query, where sim(rq, q) is the similarity between the raw query rq and the more popular query q. sim(q, title) is the maximum similarity between q and one of the topics from the title, and sim(q, text) is the maximum similarity between q and one of the topics from the text.
We rank rep score, and select the top k (k = 3 works well for our case) rep queries for each query to generate the rep query mappings.
Intent classification Rep query works well for common queries. However, for long-tail queries, rep query is usually empty, as there are not enough data points. To address this, we created a CNN-based deep learning model to identify the intent of each query.
CNN is used to capture local "spatial" patterns in data. It is useful for things that are closer together and more closely related than things far away, such as image recognition and text classification. Intents are extracted from articles. We first group articles based on intent. For instance, articles titled "Canceling Your Premium Subscription" and "Canceling or Updating a Premium Subscription Purchased on Your Apple Device" are considered to have the same intent of "cancel premium." Then, we extract all the intents from articles. 
During training time, a set of queries with intents will be loaded into CNN for deep learning.  First, each query will be transformed into a sentence matrix.  Second, CNN will use multiple filters to "scan" and do convolution through the sentence matrix, and produce feature maps.  Last, with feature maps and labeled intents, Classifier is able to group features for each intent. At the same time, Classifier will use some strategy to measure the distance between different feature maps. Feature maps with a small distance should belong to the same intent. Queries (and their feature maps) that are incorrectly categorized by Classifier will be back propagated to CNN for tuning of parameters, such as (1) convolution (number of filters, filter size, weights within each filter) and (2) pooling (window size, window stride).   
During online serving time, given a member query, CNN is able to generate features for it. Then, Classifier is able to map features to proposed intents (articles) with different probabilities.
Figure 3 shows the overall architecture of the revised Care Search system and how NLP weighs in on the system. During offline index generation, lemmatization is used to standardize the article content. During online search, the raw query will first be converted to rep query and then sent to both Galene and Intent Classifier. Here, Galene is for keyword search, while Intent Classifier is for relevance match. After that, hits from these two parts will be merged and scored further based on features such as view count, freshness, category, etc. Finally, the ranked results are returned to the member.
Click-through rate (CTR): The ratio of searches with at least one click on the result page to the number of total searches. This is to reflect if the search results are clear to members. 
"Happy path" session rate: Defined as one session where the member clicks only one article from the search results and then leaves the page without creating a case. This path is the ideal member experience we try to achieve. 
"Undesired" session rate: This metric is meant to evaluate the most undesired path, which is when a member completes a search, clicks on no articles, and creates a case directly. This metric helps us track how updates to our search algorithm affect increases in case volume, and indirectly, measure the relevance of our results. 
Based on the above metrics, we observed a significant performance increase with our new search. First, CTR improved from 39% to 69%. Furthermore, the "happy path" rate increased from 16.2% to 29%, while "undesired" path rate decreased from 6.3% to 2.8%. Both measurements were also found to be statistically significant.
Internationalization To benefit non-English-speaking members, we are currently rolling out a German deep-learning model and will work on other languages such as Portuguese, French, Spanish, and Italian as ordered by member base in those languages.
Interactive search There are situations when a follow-up interaction is needed to better understand a member's question and optimize the search result.
Ambiguity detection: To identify if a question is ambiguous or not, we use NLP to get PoS for each word. Based on our analysis, a good question should contain at least one verb and one noun, like [do something]. If there is just a verb such as "cancel," we will ask follow-up questions, such as "what do you want to [verb]"? Alternatively, if there is just a noun such as "account," we will frame the follow-up question to be "what do you want to do for [noun]?"
Step-by-step guidance: For the types of "how to" questions, it is better to return step-by-step instructions directly in the search results. To do this, we annotate articles that could contain parallel sections to cover different scenarios, such as device type or product type. A follow-up interaction would focus on specifying the member's scenario.
Spell check Typo detection and correction is essential for any successful search service. In the future, we're looking to integrate with Microsoft Bing Spell Check to correct members' typoes on the fly and return search results based on the corrected query.
This work is a multi-team effort across the Trust, Search AI Foundation, and Data Science teams. Special thanks to James Gatenby, Zack Mulgrew, Xiaofeng Wu, and Laura Dansbury from Trust; Weiwei Guo, Jaewon Yang, Huiji Gao, and Bo Long from Search AI Foundation; Zhou Jin, Xinling Dai, Rachel Zhao, and Tiger Zhang from Data Science; and Szczepan Faber and Ning Xu for reviewing this blog.
Related storyBuilding Enterprise Software on LinkedIn's Consumer Stack: Behind the Scenes of LinkedIn Talent Hub
Natural Language Processing (or NLP) is an area that is a confluence of Artificial Intelligence and linguistics. It involves intelligent analysis of written language. If you have a lot of data written in plain text and you want to automatically get some insights from it, you need to use NLP techniques. These insights could be - sentiment analysis, information extraction, information retrieval, search etc. to name a few. Check out my post on What is Natural Language Processing?
Machine Learning (or ML) is an area of Artificial Intelligence (AI) that is a set of statistical techniques for problem solving. These techniques can be applied to a wide variety of problems which are not limited to - vision based research, fraud detection, price prediction, and even NLP. In order to apply ML techniques to NLP problems, we need to usually convert the unstructured text into a structured format.
Deep Learning (which includes Recurrent Neural Networks, Convolution neural Networks and others) is a type of Machine Learning approach. It is an extension of Neural Networks. Deep Learning is used quite extensively for vision based classification (e.g. distinguishing images of airplanes from images of dogs). Deep Learning can be used for NLP tasks as well. However it is important to note that Deep Learning algorithms do not exclusively deal with text.
The image below shows graphically how NLP is related ML and Deep Learning. Deep Learning is one of the techniques in the area of Machine Learning - there are several other techniques such as Regression, K-Means, and so on.
ML and NLP have some overlap, as Machine Learning is often used for NLP tasks. LDA (Latent Dirichlet Allocation which is a Topic Modeling Algorithm) is one such example of unsupervised machine learning.
However, NLP has a strong linguistics component (not represented in the image), that requres an understanding of how we use language. The art of understanding language involves understanding humor, sarcasm, subconscious bias in text, etc. Once we can understand that is means to to be sarcastic (yeah right!) we can encode it into a machine learning algorithm to automatically discover similar patterns for us statistically.
To summarize, in order to do any NLP, you need to understand language. Language is different for different genres (research papers, blogs, twitter have different writing styles), so there is a strong component of looking at your data manually to get a fell of what it is trying to say to you, and how you- as a human would analyze it. Once you figure out what you are doing as a human reasoning system (ignoring hash tags, using smiley faces to imply sentiment), you can use a relevant ML approach to automate that process and scale it.
If you're looking for the community's opinion, check out the outstanding/best papers from the main conferences:
Assuming our goal is to cluster text at a sentence level, one approach is to use BERT pre-trained model (say large uncased)
One of the roadblocks to entity recognition for any entity type other than person, location, organization, disease, gene, drugs, and species is the absence of labeled training data.
The use of AI methods for processing natural language and for knowledge representation is a quite reasonable and promising way to describe and compare the content of documents and user requests. The feasibility of such systems has clearly been demonstrated. However, it remains important to evaluate the performance of these methods, to develop formal models, and to aim towards methods with general applicability to large numbers of documents in many different domains. (See Lenat et al., 1983 for a description of some interesting work on a widely applicable encyclopedic knowledge base.)
POS tagging was considered a fundamental part of natural language processing (NLP), which aims to computationally determine a POS tag for a token in text context. POS tagger is a useful preprocessing tool in many NLP applications such as information extraction and information retrieval (Brants, 2000; Chang et al., 2010; Kim et al., 2003; Lin et al., 1999; Salvetti et al., 2004; Zhou and Su, 2000).
POS tagging problem has been modeled with many machine learning techniques, which include HMMs (Kim et al., 2003), maximum entropy models (McCallum et al., 2000), support vector machines, and conditional random fields (Lafferty et al., 2001). Each model can have good performance after careful adjustment such as feature selection, but HMMs have the advantages of small amount of data calculation and simplicity of modeling. In Brants (2000), HMMs combined with good smoothing techniques and with handling of unknown words work better than other models. For such a sequence recognition problem, the classical EM algorithms and Viterbi algorithms for HMMs can be found by Baum and Eagon (1967), Baum and Petrie (1966), Juang and Rabiner (1993), and Rabiner (1989).
The training set in NLP is always sparse and incomplete relative to the numbers of parameters in HMMs, because emission parameters are very high degree when words are used as observations. This leaves uncertainty in HMM parameters. POS tagging is a fundamental part in NLP and is explored with a hybrid PSO–Viterbi algorithms for HMM model.
In order to use knowledge with the sequence recognition solution, the Bonus function is used to give extra score to the path with less unseen samples. This simple Bonus function returns the number of unknown state-observation pairs.
In PSO–Viterbi implementation for POS tagging, log probabilities are used and R is reduced to one dimension. The ratio r can be set to be 3 by experience that reflects high confidence for the knowledge used.
PSO–Viterbi has structural advantages. Back-off tagging is a common way in POS tagging that the later tagger processes the tokens tagged as “unknown” by the former tagger. But back-off tagging can only use one method for a single token. Our approach can use both knowledge including dictionary and morphological forms and statistics including HMM in the fitness function. It is similar to back-off tagging, but knowledge is expressed as a numerical component of the fitness value to cooperate with other method. For further performance improvement, we recommend the usage of fitness function for suffix heuristics.
This chapter discusses the natural language interface systems. Current techniques for processing natural language all rely on explicit knowledge representations and well-articulated algorithms; they are closely related to the techniques that exist for dealing with the syntax and semantics of formal languages. The succinctness and flexibility of natural language correlates with, and derives from, properties that distinguish it in a profound way from the formal languages of mathematics, logic and computer programming: its syntactic and semantic open-endedness. Considered apart from a social context of utterance, the meaning of most natural language expressions is rather indefinite; this lack of specificity gets resolved through the shared awareness of social context of the language users. Much of the work in Artificial Intelligence emphasizes this feature of natural language. Work on plan recognition shows that the interactional meaning of an utterance is often not overtly expressed but might have to be inferred on the basis of knowledge about plausible speaker goals and ways to achieve them.
In this section, we describe the proposed concept and architecture of sentiment analysis and the components which are designed in Fig. 1.
Opinion mining or sentiment analysis approach to the utilization of text analysis, natural language processing, biometric and computational linguistic to retrieve, identify, and quantify hidden information comprehensively.
First from the dataset, which was in the raw format, several features are selected which were informative and extracted informative features. We divided the extracted features into two subsets (data frame and document term matrix) that we could make cloud of words which could provide better visualization of words from the data frame; and we could apply association rule mining and clustering on the document term matrix to get association of words.
This dataset we used for our evaluation is about US Airlines and the passenger tweets. There were several features available like tweets id, airline sentiment, negative reason, airline, airline sentiment gold, negative reason gold, re-tweet count, tweet location, tweet time zone, etc. but we selected some features and especially the text features where passenger tweets were available. There are some features like airline sentiments (positive, negative), airlines, negative reasons which were useful for further analysis.
Convolution neural network model or CNN is one of the most popular models used for natural language processing. The most important advantage that this model carries is that it can mechanically detect significant characteristics by itself. CNN also proves to be proficient in calculations as well. They can be executed in any machine and bear the speciality of using special convolution and pooling operations. The term convolution represents the mathematical functionality of unification of two information sets. CNN maintains the nonlinearity feature as should be in an effective neural network. Pooling is used to reduce the dimensionality by dropping the amount of factors and hence shortening the time taken for execution. CNN is trained using backpropagation with gradient descent. There are two parts in the CNN model, namely, mining of features and categorizing them accordingly, and the convolution layers act as the major motivating force of the CNN model.
The most fundamental techniques in information retrieval involve identifying key features in objects. For example, automatic indexing and natural language processing (e.g. noun phrase extraction or object type tagging) are frequently used to automatically extract meaningful keywords or phrases from texts [80]. Texture, color, or shape-based indexing and segmentation techniques are often used to identify images [56]. For audio and video applications, voice recognition, speech recognition, and scene segmentation techniques can be used to identify meaningful descriptors in audio or video streams [96].
As part of the Illinois DLI project, we have developed a noun phrasing technique for textual document indexing [38]. Noun phrase indexing aims to identify concepts (grammatically correct noun phrases) from a collection for term indexing. It begins with a text tokenization process to separate punctuation and symbols. It follows by part-of-speech-tagging (POST) using variations of the Brill tagger and 30-plus grammatic noun phrasing rules. Figure 1 shows an example of tagged noun phrases for a simple sentence. (The system is referred to as AZ Noun Phraser.) For example, “interactive navigation” is a noun phrase that consists of an adjective (A) and a noun (N). In [38], we have shown that the noun phrasing technique produces more accurate indices for digital libraries (than inverted word indexing or N-gram indexing) and helps in concept-based retrieval. By using such a scalable natural language processing technique, digital libraries will be able to efficiently (automatically) and precisely index its own collections.
In order to develop a computer capable of dealing with a broad range of potential topics, rapid response, confidence levels in its responses, and natural language processing, a team of IBM researchers spent years developing a distributed computing system named after IBM founder Thomas J. Watson. This design was based on a cluster of commercially available IBM Power7 systems (about the size of 10 refrigerators), optimized to process thousands of simultaneous tasks at high speed. This was not an Internet search problem; Watson was not connected to the Internet during competition, instead relying on 15 TB of memory. Powering the network required to interconnect the servers and storage in Watson were two IBM J16E Ethernet switches populated with fifteen 10 GbE line cards and 1 GbE line cards, as well as three IBM J48E 1 GbE switches. These switches were an OEM of the Juniper EX8216 and EX4200 switches, respectively. The J16E switches provided a 12.4 Tbit/s fabric, capable of processing 2 billion packets per second with line rate performance. The J48E switches were enabled with a “virtual chassis” feature, which allowed the switches to be stacked using a special dedicated high-speed port (essentially an extension of the switch backplane) without reducing the bandwidth available for server attached ports. Further, this virtual chassis configuration allowed all the J48E switches to be managed from a common interface, so they could be quickly reprovisioned if needed.
Since 2006, deep learning has evolved as a class of machine learning methods with successful applications in various fields like automatic speech recognition, classification tasks, natural language processing, dimensionality reduction, object detection, motion modeling, etc. [6,11,22,25,28]. Its algorithms are based on the architecture of hierarchical explanatory factors and distribution representations where a cascade of many layers of nonlinear processing units is used for the supervised or unsupervised learning of feature representations per layer, with the layers forming a hierarchy from low-level to high-level features, in the sense of feature extraction and transformation [14]. It is inspired by some loosely established interpretation of information systems and communication patterns formulated on the human nervous system, which attempts to model high-level abstractions in data using a deep graph with multiple processing layers. Some notable architectures of deep learning include the deep belief networks, convolutional neural networks, and recurrent neural networks [5,6,23,54]. A detailed discussion of deep learning in neural networks can be found in Schmidhuber [54].
In traffic flow prediction, the use of deep learning approaches for features extraction and selection without any prior knowledge has been investigated by Huang et al. [27]. Its stack architecture used for traffic flow prediction of a single road is comprised of Restricted Boltzmann Machines (RBM) stacks constituting the DBN for the unsupervised feature learning having sigmoid regression layer on top. This came out of concern about the shallow architectures of neural network attributed to the single hidden layer, hence introducing the key idea of using greedy layer-wise training with stacked RBM and subsequent fine tuning according to the architecture of the DBN guaranteed near 3% improvements over state-of-the-art. A deep learning approach for traffic flow prediction with big data was introduced by Lv et al. [43] as a means for accurate and timely traffic flow information while considering the spatiotemporal correlation present in an intelligent transportation system. The following subsection covers the method using DBN.
Deep Belief Networks  In machine learning, DBN is a multilayered probability generative model composed of simple learning modules, so-called RBMs [23], also known as autoencoders [5], where each subnetwork's hidden layer serves as the visible layer for the next [5,24]. An RBM implies the absence of the lateral connections in the visible and hidden layers such that the random variables encoded by the hidden units are conditionally independent given the states of the visible units, and vice versa [46]. In Teh and Hinton [60], RBM is defined as “an undirected graphical model in which visible variables (v) are connected to stochastic hidden units (h) using undirected weighted connections.” The architecture of DBN can be much more efficient than shallow architectures as contained in the single latent layer of feedforward and BP-NN with many levels of non-linearity and highly-varying functions. Training the deep layer networks one layer at a time using greedy algorithm, rather than the gradient-based optimization often used with BP-NN, can guarantee a good local optimum [5], though with the BP-NN, feasible solution can be reached if starting in the neighborhood of a good local optimum. Also, BP-NN is liable to poor performance and prone to overfitting [26]. Moreover, despite the number of NNs that exist, finding one that precisely model a given training set can be an N-P complete problem [7,31,54]. It is noteworthy that with the labels provided, the DBM could provide a supervised feature learning model suitable for classification. While, considering the DBN in the classification tasks of traffic congestion with the binary RBM, we assigned the visible input unit, vi, as elements of the encoded joint distribution function. Based on Hinton and Sejnowski [22] and as further illustrated by O'Connor et al. [46], the encoded joint distribution function can be defined as
 With the model parameters, θ=(w,b(v)), hj is the states of the hidden units, wij represents the states connecting these units: namely, the visible input and hidden units; and bi(v) and bj(h) are the biases in the visible and hidden units respectively. Fig. 8.4 illustrates the DBN architecture.
An effective way to learn θ using contrastive divergence has been proposed by Hinton et al. [24]. Hence, we adapt the method that employs a greedy layer-wise algorithm associated with the RBM to the traffic data with the important factors: average speed, traffic flow, and the link-length assigned to the input units. The model establish the following stochastic update rules for the state for which lower energy state eventually attains an equilibrium given by:
 where σ(t)=11+et denotes the sigmoid function with the capability of having its unit state change to 0 and the flexibility of the network to generate samples over all possible states (v,h) based on the joint probability distribution, p(v,h|θ).
Radio policy selection is the frontrunner of CR applications that are dependent on knowing in which geopolitical region a radio is located. Other potential applications also exist. Given natural language processing capability, a CR that knows it is in Germany might offer to translate an English-speaking user's word from English into German when transmitting and could translate German words into English when receiving. Although this could be accomplished through language recognition more efficiently than through political boundary decisions, a boundary decision could be used to improve the language recognition search by prioritizing by local languages.
If a tourist is using the CR, the boundary decision engine could be used to display local points of interest such as historical markers. On a more personal note, the boundary decision engine could be correlated with the user's address book to locate friends and family in the vicinity, and the user can apply this information appropriately.
CNN is a deep neural network originally designed for image analysis. Recently, it was discovered that the CNN also has an excellent capacity in sequent data analysis such as natural language processing (Zhang, 2015). CNN always contains two basic operations, namely convolution and pooling. The convolution operation using multiple filters is able to extract features (feature map) from the data set, through which their corresponding spatial information can be preserved. The pooling operation, also called subsampling, is used to reduce the dimensionality of feature maps from the convolution operation. Max pooling and average pooling are the most common pooling operations used in the CNN. Due to the complicity of CNN, relu is the common choice for the activation function to transfer gradient in training by backpropagation.
This blog is a part of our Chief Architect's "Cruising the Data Ocean" series. It offers a deep-dive into some essential data mining tools and techniques for harvesting content from the Internet and turning it into significant business insights.
Once you have identified, extracted, and cleansed the content needed for your use case, the next step is to have an understanding of that content. In many use cases, the content with the most important information is written down in a natural language (such as English, German, Spanish, Chinese, etc.) and not conveniently tagged. To extract information from this content you will need to rely on some levels of text mining, text extraction, or possibly full-up natural language processing (NLP) techniques.
Categorizing content – positive or negative (e.g. sentiment analysis), by function, intention or purpose, or by industry or other categories for analytics and trending
Fact extraction – to fill databases with structured information for analysis, visualization, trending, or alerts
We have developed a framework to help businesses do these NLP tasks easily, accurately, and cost-efficiently. Learn more about our Saga NLU framework and request a demo.
The input to natural language processing will be a simple stream of Unicode characters (typically UTF-8). Basic processing will be required to convert this character stream into a sequence of lexical items (words, phrases, and syntactic markers) which can then be used to better understand the content.
Identify and mark sentence, phrase, and paragraph boundaries – these markers are important when doing entity extraction and NLP since they serve as useful breaks within which analysis occurs.
- Open source possibilities include the Lucene Segmenting Tokenizer and the Open NLP sentence and paragraph boundary detectors.
Language identification – will detect the human language for the entire document and for each paragraph or sentence. Language detectors are critical to determine what linguistic algorithms and dictionaries to apply to the text.
- Open source possibilities include Google Language Detector or the Optimize Language Detector or the Chromium Compact Language Detector
- API methods include Bing Language Detection API, IBM Watson Language Identification, and Google Translation API for Language Detection
Tokenization – to divide up character streams into tokens which can be used for further processing and understanding. Tokens can be words, numbers, identifiers or punctuation (depending on the use case)
- Basis Technology offers a fully featured language identification and text analytics package (called Rosette Base Linguistics) which is often a good first step to any language processing software. It contains language identification, tokenization, sentence detection, lemmatization, decompounding, and noun phrase extraction.
- Search Technologies has many of these tools available, for English and some other languages, as part of our Natural Language Processing toolkit. Our NLP tools include tokenization, acronym normalization, lemmatization (English), sentence and phrase boundaries, entity extraction (all types but not statistical), and statistical phrase extraction. These tools can be used in conjunction with the Basis Technology' solutions.
Acronym normalization and tagging – acronyms can be specified as “I.B.M.” or “IBM” so these should be tagged and normalized.
Lemmatization / Stemming – reduces word variations to simpler forms that may help increase the coverage of NLP utilities.
- Lemmatization uses a language dictionary to perform an accurate reduction to root words. Lemmatization is strongly preferred to stemming if available. Search Technologies has lemmatization for English and our partner, Basis Technologies, has lemmatization for 60 languages.
- Stemming uses simple pattern matching to simply strip suffixes of tokens (e.g. remove “s”, remove “ing”, etc.). The Open Source Lucene analyzers provide stemming for many languages.
Decompounding – for some languages (typically Germanic, Scandinavian, and Cyrillic languages), compound words will need to be split into smaller parts to allow for accurate NLP.
Entity extraction – identifying and extracting entities (people, places, companies, etc.) is a necessary step to simplify downstream processing. There are several different methods:
- Regex extraction – good for phone numbers, ID numbers (e.g. SSN, driver's licenses, etc.), e-mail addresses, numbers, URLs, hashtags, credit card numbers, and similar entities.
- Dictionary extraction – uses a dictionary of token sequences and identifies when those sequences occur in the text. This is good for known entities, such as colors, units, sizes, employees, business groups, drug names, products, brands, and so on.
- Complex pattern-based extraction – good for people names (made of known components), business names (made of known components) and context-based extraction scenarios (e.g. extract an item based on its context) which are fairly regular in nature and when high precision is preferred over high recall.
- Statistical extraction – use statistical analysis to do context extraction. This is good for people names, company names, geographic entities which are not previously known and inside of well-structured text (e.g. academic or journalistic text). Statistical extraction tends to be used when high recall is preferred over high precision.
Phrase extraction – extracts sequences of tokens (phrases) that have a strong meaning which is independent of the words when treated separately. These sequences should be treated as a single unit when doing NLP. For example, “Big Data” has a strong meaning which is independent of the words “big” and “data” when used separately. All companies have these sorts of phrases which are in common usage throughout the organization and are better treated as a unit rather than separately. Techniques to extract phrases include:
- Statistical phrase extraction - identifies token sequences which occur more frequently than expected by chance
- Hybrid - uses both techniques together and tends to be the most accurate method.
It is used for: extracting facts, entities (see above), entity relationships, actions, and metadata fields
Note that, while micro understanding generally contributes to macro understanding, the two can be entirely different. For example, a résumé (or curriculum vitae) may identify a person, overall, as a Big Data Scientist [macro understanding] but it can also identify them as being fluent in French [micro understanding].
Not all natural language understanding (NLP) projects are possible within a reasonable cost and time. After having done numerous NLP projects, we've come up with a flowchart to help you decide if your requirements are likely to be manageable with today's NLP techniques. 
Once you have decided to embark on your NLP project, if you need a more holistic understanding of the document this is a “macro understanding.” This is useful for:
Record similarity, including finding similarities between different types of records (for example, job descriptions to résumés / CVs)
A standard architecture is to use Apache Spark and Spark MLlib (Machine Learning Library) for this analysis. The NLP architecture typically looks like this:

 
In this architecture, content is downloaded from the internet or external sources (by connectors), then written to Kafka Queues and processed by Spark Machine Learning. The results are written to databases or to a search engine to be used by end-user applications.
Note that “Text Processing Libraries” will need to be included in this architecture to handle all of the basic NLP functions described above in “STEP 1:  The Basics.” This can include multiple open source projects working together, or one or two vendor packages.
Vectors – sparse vectors hold a list of weighted unique words or phrases in the document. Weights can be determined using TF/IDF or other term statistics (such as position in document, term statistics from other corpora or data sets) and then normalized
Word2Vec – computes intelligent vectors for all terms, such that similar terms have similar vectors. It can be used to find synonyms and semantically similar words.
Dimensionality Reduction – (typically, Singular Value Decomposition – SVD) used to reduce arbitrary N-length vectors into fixed vector lengths that are more amenable to classification.
DIMSUM – compares all vectors within a set to all other vectors in a set using a smart pruning algorithm. Comparisons are performed with cosine similarity.
Nearest Neighbor – a classification technique to compare vectors to sample vectors from a training set. The most similar vector (the nearest neighbor) would be used to classify the new record.
Classification Algorithms – (Decision Tree, Random Forest, Naïve Bayes, Gradient Boosted Trees) can be used to classify or categorize documents to a training set; may require that dimensions are reduced using SVD
Clustering Algorithms – (K-Means [several types], LDA, PIC) identify clusters of related documents and/or extract topics from the content set. This can be used to research the types of records in a content set or identify similar sets of documents. Note that it may be also possible to cluster users based on the types of records they like.
Logistic Regression – combine multiple document statistics and vector comparisons into a single formula for classifying a document.
Micro understanding is the extracting of individual entities, facts or relationships from the text. This is useful for (from easiest to hardest):
Extracting key entities (people, company, product, dollar amounts, locations, dates). Note that extracting “key” entities is not the same as extracting “all” entities (there is some discrimination implied in selecting what entity is 'key')
Extracting facts and metadata from full text when it's not separately tagged in the web page
Micro understanding must be done with syntactic analysis of the text. This means that order and word usage are important.
1.  Top Down – determine Part of Speech, then understand and diagram the sentence into clauses, nouns, verbs, object and subject, modifying adjectives and adverbs, etc., then traverse this structure to identify structures of interest
Disadvantages – hard to construct rules, brittle, often fails with variant input, may still require substantial pattern matching even after parsing.
Sample top-down output from Google Cloud Natural Language API(Right-click on the image and select "Open image in new tab" for better image clarity)
In the deep understanding graph, notice how all of the modifiers are linked together. Also notice that a second step (which requires custom programming) is required to take this graph and identify object / action relationships suitable for exporting to a graph or relational database.
2.  Bottoms Up – create lots of patterns, match the patterns to the text and extract the necessary facts. Patterns may be manually entered or may be computed using text mining.
Advantages – Easy to create patterns, can be done by business users, does not require programming, easy to debug and fix, runs fast, matches directly to desired outputs
3.  Statistical – similar to bottoms-up, but matches patterns against a statistically weighted database of patterns generated from tagged training data.
Disadvantages – requires generating extensive training data (1000's of examples), will need to be periodically retrained for best accuracy, cannot match on newly invented constructs, harder to debug
Note that these patterns may be entered manually, or they may be derived statistically (and weighted statistically) using training data or inferred using text mining and machine learning.
Open NLP – has many components; is complex to work with; parsing is done with the “top down” approach
UIMA – has many components and statistical annotation; tends to require a lot of programming; lends itself to a bottoms-up / statistical approach, but not easily implemented
GATE – configurable bottoms-up approach; is much easier to work with, but configurations must still be created by programmers (not business users)
Search Technologies' Natural Language Processing framework – bottoms-up approach scaled to very large sets of patterns. Patterns can be created by business users. Our framework is expected to include statistical patterns from training sets. This is in development.
Co-reference resolution - sentences often refer to previous objects. This can include the references below. In all of these cases, the desired data refers to a previous, more explicitly defined entity. To achieve the highest possible coverage, your software will need to identify these back references and resolve them.
- Partial reference: “Linda Nelson is a top accountant working in Hawaii. Linda is 49 years old.”
- Implied container reference: “The state of Maryland is a place of history. The capital, Annapolis, was founded in 1649.”
- For example: “The largest cities in Maryland are Baltimore, Columbia, Germantown, Silver Spring, and Waldorf.”
- Such lists often break NLP algorithms and may require special handling which exists outside the standard structures.
- Make sure that NLP does not match sentences and patterns across structural boundaries. For example, from one bullet point and into the next.
- Make sure that markup does not break NLP analysis where it shouldn't. For example, embedded emphasis should not cause undue problems.
At some point, someone will point to a piece of data produced by your system and say: “That looks wrong. Where did it come from?”
Acquiring content from the Internet and then extracting information from that content will likely involve many steps and a large number of computational stages. It is important to provide traceability (provenance) for all outputs so that you can carefully trace back through the system to identify exactly how that information came to be.
Save the start and end character positions of all blocks of text extracted from the web page
Save the start and end character positions for all entities, plus the entity ID and entity type ID matched
Save the start and end character positions for all patterns matched, plus the pattern ID and sub-pattern IDs (for nested or recursive patterns)
By saving this information throughout the process, you can trace back from the outputs all the way back to the original web page or file which provided the content that was processed. This will allow you to answer the question “Where did this come from?” with perfect accuracy, and will also make it possible to do quality analysis at every step.
Many of these processes can be mind-numbingly repetitive. In a large-scale system, you will need to consider the human element and build that into your NLP system architecture.
Creating user interfaces to simplify and guide the human evaluation process, for example, allowing users to easily tag entities in content using a WYSIWYG tool and providing easily editable lists to review (with sortable statistics and easy character searching)
Finding ways to incorporate human review / human-in-the-loop as part of the standard business process, for example, pre-filling out a form using extracted understanding and having the employee review it before clicking “save” and uploading new content
Once you have extracted information using NLP techniques, how do you use the results for your business needs? I'll discuss this step in my next post.
If you are working on your NLP project and want to learn more about how we can help you leverage these techniques or our Saga NLU framework, contact us for further discussions.    
We're very excited to announce that we're now part of Accenture! Read the announcement here.
The field of study that focuses on the interactions between human language and computers is called Natural Language Processing, or NLP for short. It sits at the intersection of computer science, artificial intelligence, and computational linguistics (Wikipedia).
“Nat­ur­al Lan­guage Pro­cessing is a field that cov­ers com­puter un­der­stand­ing and ma­nip­u­la­tion of hu­man lan­guage, and it's ripe with pos­sib­il­it­ies for news­gath­er­ing,” Anthony Pesce said in Natural Language Processing in the kitchen. “You usu­ally hear about it in the con­text of ana­lyz­ing large pools of legis­la­tion or other doc­u­ment sets, at­tempt­ing to dis­cov­er pat­terns or root out cor­rup­tion.”
NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way. By utilizing NLP, developers can organize and structure knowledge to perform tasks such as automatic summarization, translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation.
“Apart from common word processor operations that treat text like a mere sequence of symbols, NLP considers the hierarchical structure of language: several words make a phrase, several phrases make a sentence and, ultimately, sentences convey ideas,” John Rehling, an NLP expert at Meltwater Group, said in How Natural Language Processing Helps Uncover Social Media Sentiment. “By analyzing language for its meaning, NLP systems have long filled useful roles, such as correcting grammar, converting speech to text and automatically translating between languages.”
NLP is used to analyze text, allowing machines to understand how human's speak. This human-computer interaction enables real-world applications like automatic text summarization, sentiment analysis, topic extraction, named entity recognition, parts-of-speech tagging, relationship extraction, stemming, and more. NLP is commonly used for text mining, machine translation, and automated question answering.
NLP is characterized as a hard problem in computer science. Human language is rarely precise, or plainly spoken. To understand human language is to understand not only the words, but the concepts and how they're linked together to create meaning. Despite language being one of the easiest things for humans to learn, the ambiguity of language is what makes natural language processing a difficult problem for computers to master.
NLP algorithms are typically based on machine learning algorithms. Instead of hand-coding large sets of rules, NLP can rely on machine learning to automatically learn these rules by analyzing a set of examples (i.e. a large corpus, like a book, down to a collection of sentences), and making a statical inference. In general, the more data analyzed, the more accurate the model will be.
Summarize blocks of text using Summarizer to extract the most important and central ideas while ignoring irrelevant information. 
Create a chat bot using Parsey McParseface, a language parsing deep learning model made by Google that uses Point-of-Speech tagging.
Automatically generate keyword tags from content using AutoTag, which leverages LDA, a technique that discovers topics contained within a body of text.
Identify the type of entity extracted, such as it being a person, place, or organization using Named Entity Recognition.
Use Sentiment Analysis to identify the sentiment of a string of text, from very negative to neutral to very positive.
Reduce words to their root, or stem, using PorterStemmer, or break up text into tokens using Tokenizer.
These libraries provide the algorithmic building blocks of NLP in real-world applications. Algorithmia provides a free API endpoint for many of these algorithms, without ever having to setup or provision servers and infrastructure.
Apache OpenNLP: a machine learning toolkit that provides tokenizers, sentence segmentation, part-of-speech tagging, named entity extraction, chunking, parsing, coreference resolution, and more.
Natural Language Toolkit (NLTK): a Python library that provides modules for processing text, classifying, tokenizing, stemming, tagging, parsing, and more.
Standford NLP: a suite of NLP tools that provide part-of-speech tagging, the named entity recognizer, coreference resolutionsystem, sentiment analysis, and more.
MALLET: a Java package that provides Latent Dirichlet Allocation, document classification, clustering, topic modeling, information extraction, and more.
Use Summarizer to automatically summarize a block of text, exacting topic sentences, and ignoring the rest.
Generate keyword topic tags from a document using LDA (Latent Dirichlet Allocation), which determines the most relevant words from a document. This algorithm is at the heart of the Auto-Tag and Auto-Tag URL microservices.
Sentiment Analysis, based on StanfordNLP, can be used to identify the feeling, opinion, or belief of a statement, from very negative, to neutral, to very positive. Often, developers with use an algorithm to identify the sentiment of a term in a sentence, or use sentiment analysis to analyze social media.
NLP algorithms can be extremely helpful for web developers, providing them with the turnkey tools needed to create advanced applications, and prototypes.
Social media analysis is a great example of NLP use. Brands track conversations online to understand what customers are saying, and glean insight into user behavior.
“One of the most compelling ways NLP offers valuable intelligence is by tracking sentiment — the tone of a written message (tweet, Facebook update, etc.) — and tag that text as positive, negative or neutral,” Rehling said.
Start by using the algorithm Retrieve Tweets With Keyword to capture all mentions of your brand name on Twitter. In our case, we search for mentions of Algorithmia.
Then, pipe the results into the Sentiment Analysis algorithm, which will assign a sentiment rating from 0-4 for each string (Tweet).
“Hashtags and topics are two different ways of grouping and participating in conversations,” Chris Struhar, a software engineer on News Feed, said in How Facebook Built Trending Topics With Natural Language Processing. “So don't think Facebook won't recognize a string as a topic without a hashtag in front of it. Rather, it's all about NLP: natural language processing. Ain't nothing natural about a hashtag, so Facebook instead parses strings and figures out which strings are referring to nodes — objects in the network. We look at the text, and we try to understand what that was about.”
It's not just social media that can use NLP to it's benefit. Publishers are hoping to use NLP to improve the quality of their online communities by leveraging technology to “auto-filter the offensive comments on news sites to save moderators from what can be an 'exhausting process',” Francis Tseng said in Prototype winner using 'natural language processing' to solve journalism's commenting problem.
Other practical uses of NLP include monitoring for malicious digital attacks, such as phishing, or detecting when somebody is lying.
You can build a machine learning RSS reader in less than 30-minutes using the follow algorithms:
Sentiment Analysis is then used to identify if the article is positive, negative, or neutral.
Speech and Language Processing: “The first of its kind to thoroughly cover language technology – at all levels and with all modern technologies – this book takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations.”
Foundations of Statistical Natural Language Processing: “This foundational text is the first comprehensive introduction to statistical natural language processing (NLP) to appear. The book contains all the theory and algorithms needed for building NLP tools. It provides broad but rigorous coverage of mathematical and linguistic foundations, as well as detailed discussion of statistical methods, allowing students and researchers to construct their own implementations. The book covers collocation finding, word sense disambiguation, probabilistic parsing, information retrieval, and other applications.”
Handbook of Natural Language Processing: “The Second Edition presents practical tools and techniques for implementing natural language processing in computer systems. Along with removing outdated material, this edition updates every chapter and expands the content to include emerging areas, such as sentiment analysis.”
Statistical Language Learning (Language, Speech, and Communication): “Eugene Charniak breaks new ground in artificial intelligenceresearch by presenting statistical language processing from an artificial intelligence point of view in a text for researchers and scientists with a traditional computer science background.”
Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit
“This is a book about Natural Language Processing. By “natural language” we mean a language that is used for everyday communication by humans; languages like English, Hindi or Portuguese. At one extreme, it could be as simple as counting word frequencies to compare different writing styles.
Speech and Language Processing, 2nd Edition 2nd Edition
“An explosion of Web-based language techniques, merging of distinct fields, availability of phone-based dialogue systems, and much more make this an exciting time in speech and language processing. The first of its kind to thoroughly cover language technology – at all levels and with all modern technologies – this text takes an empirical approach to the subject, based on applying statistical and other machine-learning algorithms to large corporations. The authors cover areas that traditionally are taught in different courses, to describe a unified vision of speech and language processing.”
Introduction to Information Retrieval
“As recently as the 1990s, studies showed that most people preferred getting information from other people rather than from information retrieval systems. However, during the last decade, relentless optimization of information retrieval effectiveness has driven web search engines to new quality levels where most people are satisfied most of the time, and web search has become a standard and often preferred source of information finding. For example, the 2004 Pew Internet Survey (Fallows, 2004) found that 92% of Internet users say the Internet is a good place to go for getting everyday information.” To the surprise of many, the field of information retrieval has moved from being a primarily academic discipline to being the basis underlying most people's preferred means of information access.”
Natural Language Processing Tutorial: “We will go from tokenization to feature extraction to creating a model using a machine learning algorithm. You can get the source of the post from github.”
Basic Natural Language Processing: “In this tutorial competition, we dig a little “deeper” into sentiment analysis. People express their emotions in language that is often obscured by sarcasm, ambiguity, and plays on words, all of which could be very misleading for both humans and computers.“
An NLP tutorial with Roger Ebert: “Natural Language Processing is the process of extracting information from text and speech. In this post, we walk through different approaches for automatically extracting information from text—keyword-based, statistical, machine learning—to explain why many organizations are now moving towards the more sophisticated machine-learning approaches to managing text data.”
If you're interested in learning more, this free introductory course from Stanford University will help you will learn the fundamentals of natural language processing, and how you can use it to solve practical problems.
Once you've gotten the fundamentals down, apply what you've learned using Python and NLTK, the most popular framework for Python NLP.
Stanford Natural Language Processing on Coursera
“This course covers a broad range of topics in natural language processing, including word and sentence tokenization, text classification and sentiment analysis, spelling correction, information extraction, parsing, meaning extraction, and question answering, We will also introduce the underlying theory from probability, statistics, and machine learning that are crucial for the field, and cover fundamental algorithms like n-gram language modeling, naive bayes and maxent classifiers, sequence models like Hidden Markov Models, probabilistic dependency and constituent parsing, and vector-space models of meaning.”
Stanford Machine Learning on Coursera
“Machine learning is the science of getting computers to act without being explicitly programmed. Many researchers also think it is the best way to make progress towards human-level AI. In this class, you will learn about the most effective machine learning techniques, and gain practice implementing them and getting them to work for yourself.”
Udemy's Introduction to Natural Language Processing
“This course introduces Natural Language Processing through the use of python and the Natural Language Tool Kit. Through a practical approach, you'll get hands on experience working with and analyzing text. As a student of this course, you'll get updates for free, which include lecture revisions, new code examples, and new data projects.”
Certificate in Natural Language Technology
“When you talk to your mobile device or car navigation system – or it talks to you – you're experiencing the fruits of developments in natural language processing. This field, which focuses on the creation of software that can analyze and understand human languages, has grown rapidly in recent years and now has many technological applications. In this three-course certificate program, we'll explore the foundations of computational linguistics, the academic discipline that underlies NLP.”
Natural language processing (Wikipedia): “Natural language processing (NLP) is a field of computer science, artificial intelligence, and computational linguistics concerned with the interactions between computers and human (natural) languages. In 1950, Alan Turing published an article titled 'Computing Machinery and Intelligence' which proposed what is now called the Turing test as a criterion of intelligence. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing.”
Outline of natural language processing (Wikipedia): “The following outline is provided as an overview of and topical guide to natural language processing: Natural language processing – computer activity in which computers are entailed to analyze, understand, alter, or generate natural language.”
Apache OpenNLP: “The Apache OpenNLP library is a machine learning based toolkit for the processing of natural language text.”
Natural Language Toolkit: “NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum. Natural Language Processing with Python provides a practical introduction to programming for language processing.”
Algorithmia AI Cloud is built to scale. You write the code and compose the workflow. We take care of the rest.
In this article well be learning about Natural Language Processing(NLP) which can help computers analyze text easily i.e detect spam emails, autocorrect. We'll see how NLP tasks are carried out for understanding human language.
NLP is a field in machine learning with the ability of a computer to understand, analyze, manipulate, and potentially generate human language.
(Natural Language Toolkit)NLTK: NLTK is a popular open-source package in Python. Rather than building all tools from scratch, NLTK provides all common NLP Tasks.
Type !pip install nltk in the Jupyter Notebook or if it doesn't work in cmd type conda install -c conda-forge nltk. This should work in most cases.
After typing the above, we get an NLTK Downloader Application which is helpful in NLP Tasks
Stopwords Corpus is already installed in my system which helps in removing redundant repeated words. Similarly, we can install other useful packages.
While reading data, we get data in the structured or unstructured format. A structured format has a well-defined pattern whereas unstructured data has no proper structure. In between the 2 structures, we have a semi-structured format which is a comparably better structured than unstructured format.
As we can see from above when we read semi-structured data it is hard to interpret so we use pandas to easily understand our data.
Cleaning up the text data is necessary to highlight attributes that we're going to want our machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:
Punctuation can provide grammatical context to a sentence which supports our understanding. But for our vectorizer which counts the number of words and not the context, it does not add value, so we remove all special characters. eg: How are you?->How are you
Tokenizing separates text into units such as sentences or words. It gives structure to previously unstructured text. eg: Plata o Plomo-> 'Plata','o','Plomo'.
Stopwords are common words that will likely appear in any text. They don't tell us much about our data so we remove them. eg: silver or lead is fine for me-> silver, lead, fine.
Stemming helps reduce a word to its stem form. It often makes sense to treat related words in the same way. It removes suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach. It reduces the corpus of words but often the actual words get neglected. eg: Entitling,Entitled->EntitlNote: Some search engines treat words with the same stem as synonyms.
Lemmatizing derives the canonical form ('lemma') of a word. i.e the root form. It is better than stemming as it uses a dictionary-based approach i.e a morphological analysis to the root word.eg: Entitling, Entitled->Entitle
In Short, Stemming is typically faster as it simply chops off the end of the word, without understanding the context of the word. Lemmatizing is slower and more accurate as it takes an informed analysis with the context of the word in mind.
In body_text_stemmed, we can words like chances are lemmatized to chance whereas it is stemmed to chanc.
Vectorizing is the process of encoding text as integers i.e. numeric form to create feature vectors so that machine learning algorithms can understand our data.
Bag of Words (BoW) or CountVectorizer describes the presence of words within the text data. It gives a result of 1 if present in the sentence and 0 if not present. It, therefore, creates a bag of words with a document-matrix count in each text document.
BOW is applied on the body_text, so the count of each word is stored in the document matrix. (Check the repo).
N-grams are simply all combinations of adjacent words or letters of length n that we can find in our source text. Ngrams with n=1 are called unigrams. Similarly, bigrams (n=2), trigrams (n=3) and so on can also be used.
Unigrams usually don't contain much information as compared to bigrams and trigrams. The basic principle behind n-grams is that they capture the letter or word is likely to follow the given word. The longer the n-gram (higher n), the more context you have to work with.
N-Gram is applied on the body_text, so the count of each group words in a sentence word is stored in the document matrix. (Check the repo).
It computes “relative frequency” that a word appears in a document compared to its frequency across all documents. It is more useful than “term frequency” for identifying “important” words in each document (high frequency in that document, low frequency in other documents).Note: Used for search engine scoring, text summarization, document clustering.
Check my previous post — In the TF-IDF Section, I have elaborated on the working of TF-IDF.
TF-IDF is applied on the body_text, so the relative count of each word in the sentences is stored in the document matrix. (Check the repo).
Note: Vectorizers outputs sparse matrices. Sparse Matrix is a matrix in which most entries are 0. In the interest of efficient storage, a sparse matrix will be stored by only storing the locations of the non-zero elements.
Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. It is like an art as it requires domain knowledge and it can tough to create features, but it can be fruitful for ML algorithm to predict results as they can be related to the prediction.
We can clearly see that Spams have a high number of words as compared to Hams. So it's a good feature to distinguish.
Spam has a percentage of punctuations but not that far away from Ham. Surprising as at times spam emails can contain a lot of punctuation marks. But still, it can be identified as a good feature.
We use an ensemble method of machine learning where multiple models are used and their combination produces better results than a single model(Support Vector Machine/Naive Bayes). Ensemble methods are the first choice for many Kaggle Competitions. Random Forest i.e multiple random decision trees are constructed and the aggregates of each tree are used for the final prediction. It can be used for classification as well as regression problems. It follows a bagging strategy where randomly.
Grid-search: It exhaustively searches overall parameter combinations in a given grid to determine the best model.
Cross-validation: It divides a data set into k subsets and repeat the method k times where a different subset is used as the test set i.e in each iteration.
The mean_test_score for n_estimators =150 and max_depth gives the best result. Where n_estimators is the number of trees in the forest.(group of decision trees) and max_depth is the max number of levels in each decision tree.
Improvements: We can use GradientBoosting, XgBoost for classifying. I tried GridSearchCV on GradientBoosting, results were taking a lot of time so scrapped the idea of including here. It takes a lot of time as GradientBoosting takes an iterative approach of combining weak learners to create strong learner by focusing on mistakes of prior iteration. In short, compared to Random Forest it follows a sequential approach rather than random parallel approach.
Random Forest gives an accuracy of 97.7%. High-value F1-score is also obtained from the model. Confusion Matrix tells us that we correctly predicted 965 hams and 123 spams.0 hams were incorrectly identified as spams and 26 spams were incorrectly predicted as hams. Detecting spams as hams are justifiable as compared to hams as spams.
In summary, we learned how to perform basic NLP tasks and used a machine learning classifier to predict whether the SMS is Spam or Ham.
Natural Language Processing (NLP) is a branch of AI that helps computers to understand, interpret and manipulate human language.  
NLP helps developers to organize and structure knowledge to perform tasks like translation, summarization, named entity recognition, relationship extraction, speech recognition, topic segmentation, etc.

NLP is a way of computers to analyze, understand and derive meaning from a human languages such as English, Spanish, Hindi, etc.



In this nlp tutorial, you will learn: 

What is Natural Language Processing?    			
History of NLP     
How does NLP work?    
Components of NLP     
NLP and writing systems     
How to implement NLP     
NLP Examples     
Future of NLP     
Natural language vs. Computer Language     
Advantages of NLP     
Disadvantages of NLP     

History of NLP
Here, is are important events in the history of Natural Language Processing:

1950- NLP started when Alan Turing published an article called "Machine and Intelligence."
1950- Attempts to automate translation between Russian and English 
1960- The work of Chomsky and others on formal language theory and generative syntax 
1990- Probabilistic and data-driven models had become quite standard
2000- A Large amount of spoken and textual data become available 
How does NLP work?
Before we learn how NLP works, let's understand how humans use language-

Every day, we say thousand of a word that other people interpret to do countless things. We, consider it as a simple communication, but we all know that words run much deeper than that. There is always some context that we derive from what we say and how we say it., NLP never focuses on voice modulation; it does draw on contextual patterns. 

Example: 

Man is to woman as king is to __________?
Meaning (king) – meaning (man) + meaning ( woman)=?
The answer is-  queen

Here, we can easily co-relate because man is male gender and woman is female gender. In the same way, the king is masculine gender, and its female gender is queen. 

Example: 

Is King to kings as the queen is to_______?
The answer is--- queens 

Here, we can see two words kings and kings where one is singular and other is plural. Therefore, when the world queen comes, it automatically co-relates with queens again singular plural. 

Here, the biggest question is that how do we know what words mean? Let's, say who will call it queen?


The answer is we learn this thinks through experience. However, here the main question is that how computer know about the same?
We need to provide enough data for Machines to learn through experience. We can feed details like





Her Majesty the Queen.The Queen's speech during the State visitThe crown of Queen ElizabethThe Queens's MotherThe queen  is generous. 
With above examples the machine understands the entity Queen. 

The machine creates word vectors as below. A word vector is built using surrounding words. 


The machine creates these vectors 
As it learns  from multiple  datasetsUse Machine learning (e.g., Deep Learning algorithms)A word vector is built using surrounding words. 
Here is the formula: 
Meaning (king) – meaning (man) + meaning (woman)=?
This amounts to performing simple algebraic operations on word vectors:
Vector ( king) – vector (man) + vector (woman)= vector(?)
To which the machine answers queen. 

Components of NLP 
Five main Component of Natural Language processing are:

    Morphological and Lexical Analysis
    Syntactic Analysis
    Semantic Analysis
    Discourse Integration 
    Pragmatic Analysis

Morphological and Lexical Analysis
Lexical analysis is a vocabulary that includes its words and expressions. It depicts analyzing, identifying and description of the structure of words. It includes dividing a text into paragraphs, words and the sentences
Individual words are analyzed into their components, and nonword tokens such as punctuations are separated from the words.

Semantic Analysis
Semantic Analysis is a structure created by the syntactic analyzer which assigns meanings.  This component transfers linear sequences of words into structures. It shows how the words are associated with each other.
Semantics focuses only on the literal meaning of words, phrases, and sentences. This only abstracts the dictionary meaning or the real meaning from the given context. The structures assigned by the syntactic analyzer always have assigned meaning
E.g.. "colorless green idea." This would be rejected by the Symantec analysis as colorless Here; green doesn't make any sense. 

Pragmatic Analysis
Pragmatic Analysis deals with the overall communicative and social content and its effect on interpretation. It means abstracting or deriving the meaningful use of language in situations. In this analysis, the main focus always on what was said in reinterpreted on what is meant. 
Pragmatic analysis helps users to discover this intended effect by applying a set of rules that characterize cooperative dialogues.
E.g., "close the window?" should be interpreted as a request instead of an order.

Syntax analysis 
The words are commonly accepted as being the smallest units of syntax. The syntax refers to the principles and rules that govern the sentence structure of any individual languages.
Syntax focus about the proper ordering of words which can affect its meaning. This involves analysis of the words in a sentence by following the grammatical structure of the sentence. The words are transformed into the structure to show hows the word are related to each other. 

Discourse Integration
It means a sense of the context. The meaning of any single sentence which depends upon that sentences. It also considers the meaning of the following sentence.
For example, the word "that" in the sentence "He wanted that" depends upon the prior discourse context.


NLP and writing systems
The kind of writing system used for a language is one of the deciding factors in determining the best approach for text pre-processing. Writing systems can be 





Logographic: a Large number of individual symbols represent words. Example Japanese, MandarinSyllabic:  Individual symbols represent syllablesAlphabetic: Individual symbols represent sound
Majority of the writing systems use the Syllabic or Alphabetic system. Even English, with its relatively simple writing system based on the Roman alphabet, utilizes logographic symbols which include Arabic numerals, Currency symbols (S, £), and other special symbols. 

This pose following challenges

Extracting meaning(semantics) from a text is a challengeNLP is dependent on the quality of the corpus. If the domain is vast, it's difficult to understand context. There is a dependence on the character set and language
How to implement NLP
Below, given are popular methods used for Natural Learning Process:

Machine learning:  The learning nlp procedures used during machine learning. It automatically focuses on the most common cases. So when we write rules by hand, it is often not correct at all concerned about human errors.

Statistical inference: NLP can make use of statistical inference algorithms. It helps you to produce models that are robust. e.g., containing words or structures which are known to everyone. 

NLP Examples
Today, Natual process learning technology is widely used technology. 
Here, are common Application' of NLP:

Information retrieval & Web Search
Google, Yahoo, Bing, and other search engines base their machine translation technology on NLP deep learning models. It allows algorithms to read text on a webpage, interpret its meaning and translate it to another language.
Grammar Correction:
NLP technique is widely used by word processor software like MS-word for spelling correction & grammar check. 


Question Answering
Type in keywords to ask Questions in Natural Language. 


Text Summarization
The process of summarising important information from a source to produce a shortened version

Machine Translation
Use of computer applications to translate text or speech from one natural language to another.


Sentiment analysis
NLP helps companies to analyze a large number of reviews on a product. It also allows their customers to give a review of the particular product. 

Future of NLP 
Human readable natural language processing is the biggest Al- problem.  It is all most same as solving the central artificial intelligence problem and making computers as intelligent as people.
Future computers or machines with the help of NLP  will able to learn from the information online and apply that in the real world, however, lots of work need to on this regard.
  Naturla language toolkit or nltk become  more effectiveCombined with natural language generation, computers will become more capable of receiving and giving useful and resourceful information or data.
Natural language vs. Computer Language
Parameter
Natural Language
Computer Languages
Ambiguous
They are ambiguous in nature. 
They are designed to unambiguous.
Redundancy
Natural languages employ lots of redundancy.
Formal languages are less redundant. 
Literalness
Natural languages are made of idiom & metaphor
Formal languages mean exactly what they want to say

Advantages of NLP 
Users can ask questions about any subject and get a direct response within seconds. 
NLP system provides answers to the questions in natural language
NLP system offers exact answers to the questions, no unnecessary or unwanted information
The accuracy of the answers increases with the amount of relevant information provided in the question. 
NLP process helps computers communicate with humans in their language and scales other language-related tasks 				
Allows you to perform more language-based data compares to a human being without fatigue and in an unbiased and consistent way.
Structuring a highly unstructured data source


Disadvantages of NLP
Complex Query Language- the system may not be able to provide the correct answer it the question that is poorly worded or ambiguous.
The system is built for a single and specific task only; it is unable to adapt to new domains and problems because of limited functions.
NLP system doesn't have a user interface which lacks features that allow users to further interact with the system
Summary
Natural Language Processing is a branch of AI which helps computers to understand, interpret and manipulate human languageNLP started when Alan Turing published an article called "Machine and Intelligence".
NLP never focuses on voice modulation; it does draw on contextual patterns
Five essential components of Natural Language processing are 1) Morphological and Lexical Analysis 2)Syntactic Analysis 3) Semantic Analysis 4) Discourse Integration 5) Pragmatic Analysis
Three types of the Natural process writing system are 1)Logographic 2) Syllabic 3) Alphabetic
Machine learning and Statistical inference are two methods to implementation of Natural Process Learning
Essential Applications of NLP are Information retrieval & Web Search, Grammar Correction Question Answering, , Text Summarization, Machine Translation, etc.
Future computers or machines with the help of NLP and Data Science will able to learn from the information online and apply that in the real world, however, lots of work need to on this regard
NLP is are ambiguous while  open source computer language  is designed to unambiguous
The biggest advantage of the NLP system is that it offers exact answers to the questions, no unnecessary or unwanted information
The biggest draw back of the NLP system is built for a single and specific task only so it is unable to adapt to new domains and problems because of limited functions

  
What is Data Reconciliation? 

 Data reconciliation (DR) is defined as a process of verification of...
1) What is Android? 		 
 It is an open-sourced operating system that is used primarily on mobile...
Today's market is flooded with an array of Big Data tools. They bring cost efficiency, better time...
1: What is a shell? 		 
 Shell is an interface between the user and the kernel. Even though there can...
What is Python 2? 

 Python 2 made code development process easier than earlier versions. It...
Natural Language Processing (NLP) is a field of artificial intelligence that enables computers to analyze and understand human language. It was formulated to build software that generates and comprehends natural languages, so that a user can have natural conversations with his or her computer instead of through programming or artificial languages like Java or C.
Natural Language Processing (NLP) is one step in a larger mission for the technology sector – namely, to use artificial intelligence (AI) to simplify the way the world works. The digital world has proved to be a game-changer for a lot of companies as an increasingly technology-savvy population finds new ways of interacting online with each other and with companies. Social media has redefined the meaning of community; cryptocurrency has changed the digital payment norm; e-commerce has created a new meaning of the word convenience; and cloud storage has introduced another level of data retention to the masses.
Through AI, fields like machine learning and deep learning are opening eyes to a world of all possibilities. Machine learning is increasingly being used in data analytics to make sense of big data. It is also used to program chatbots to simulate human conversations with customers. However, these forward applications of machine learning wouldn't be possible without the improvisation of Natural Language Processing (NLP).
NLP combines AI with computational linguistics and computer science to process human or natural languages and speech. The process can be broken down into three parts. The first task of NLP is to understand the natural language received by the computer. The computer uses a built-in statistical model to perform a speech recognition routine that converts the natural language to a programming language. It does this by breaking down a recent speech it hears into tiny units, and then compares these units to previous units from a previous speech. The output or result in text format statistically determines the words and sentences that were most likely said. This first task is called the speech-to-text process.
The next task is called the part-of-speech (POS) tagging or word-category disambiguation. This process elementarily identifies words in their grammatical forms as nouns, verbs, adjectives, past tense, etc. using a set of lexicon rules coded into the computer. After these two processes, the computer probably now understands the meaning of the speech that was made.
The third step taken by an NLP is text-to-speech conversion. At this stage, the computer programming language is converted into an audible or textual format for the user. A financial news chatbot, for example, that is asked a question like “How is Google doing today?” will most likely scan online finance sites for Google stock, and may decide to select only information like price and volume as its reply.
NLP attempts to make computers intelligent by making humans believe they are interacting with another human. The Turing test, proposed by Alan Turing in 1950, states that a computer can be fully intelligent if it can think and make a conversation like a human without the human knowing he or she is conversing with a machine. So far, only one computer has passed the test – a chatbot with the persona of a 13-year-old boy. This is not to say that an intelligent machine is impossible to build, but it does outline the difficulties inherent in making a computer think or converse like a human. Since words can be used in different contexts, and machines don't have the real life experience that humans have for conveying and describing entities in words, it may take a little while longer before the world can completely do away with computer programming language.
Master the skills to get computers to understand, process, and manipulate human language. Build models on real data, and get hands-on experience with sentiment analysis, machine translation, and more.
Learn text processing fundamentals, including stemming and lemmatization. Explore machine learning methods in sentiment analysis. Build a speech tagging model.Part of Speech Tagging
Learn advanced techniques like word embeddings, deep learning attention, and more. Build a machine translation model using recurrent neural network architectures.Machine Translation
Learn voice user interface techniques that turn speech into text and vice versa. Build a speech recognition model using deep neural networks.Speech Recognizer
We recommend our Deep Learning Nanodegree program as the perfect starting point for your deep learning education.
Luis was formerly a Machine Learning Engineer at Google. He holds a PhD in mathematics from the University of Michigan, and a Postdoctoral Fellowship at the University of Quebec at Montreal. 
Jay has a degree in computer science, loves visualizing machine learning concepts, and is the Investment Principal at STV, a $500 million venture capital fund focused on high-technology startups.
Arpan is a computer scientist with a PhD from North Carolina State University. He teaches at Georgia Tech (within the Masters in Computer Science program), and is a coauthor of the book Practical Graph Mining with R.
Dana is an electrical engineer with a Masters in Computer Science from Georgia Tech. Her work experience includes software development for embedded systems in the Automotive Group at Motorola, where she was awarded a patent for an onboard operating system.
 Average salary increase for graduates who found a new, better job within six months of graduation. 
Why should I enroll in this program?This program offers a deep dive into modern Natural Language Process techniques. Mastering these skills will prepare you to build applications involving written and spoken language. We've collaborated with leading innovators such as IBM and Amazon to create our world-class curriculum, and you'll learn from an instructor team comprised of experts from both Udacity and industry professionals. Massive growth is being predicted for the Natural Language Processing software market, making now the perfect time to enter this field.
What jobs will this program prepare me for?In this program, you'll develop and refine specialized skills in natural language processing and voice user interfaces. The curriculum is not designed to prepare you for a specific job; instead, the goal is that you'll expand your skills in the natural language processing domain. Growth predictions are extremely high for this market, and having these in-demand skills will significantly enhance your ability to advance your AI career.
How do I know if this program is right for me?If your goal is to become an expert in Natural Language Processing (NLP), this program is ideal for you. Over the course of this program, you'll become an expert in the main components of NLP, including speech recognition, sentiment analysis, and machine translation. You'll learn cutting edge probabilistic and deep learning models, code them and train them on real data, and build a career-ready portfolio as an NLP expert!
Do I need to apply? What are the admission criteria?No. This Nanodegree program accepts all applicants regardless of experience and specific background.
What are the prerequisites for enrollment?To succeed in this Nanodegree program, we recommend you first take any course in Deep Learning equivalent to our Deep Learning Nanodegree program. You also need to be able to communicate fluently and professionally in written and spoken English.
Additionally, you should have the following knowledge:
Intermediate Python programming knowledge, including:

Strings, numbers, and variablesStatements, operators, and expressionsLists, tuples, and dictionariesConditions & loopsGenerators & comprehensionsProcedures, objects, modules, and librariesTroubleshooting and debuggingResearch & documentationProblem solvingAlgorithms and data structures
Basic shell scripting:

Run programs from a command lineDebug error messages and feedbackSet environment variablesEstablish remote connections
Basic statistical knowledge, including:

Populations, samplesMean, median, modeStandard errorVariation, standard deviationsNormal distribution
Intermediate differential calculus and linear algebra, including:

Derivatives & IntegralsSeries expansionsMatrix operations through eigenvectors and eigenvalues
If I do not meet the requirements to enroll, what should I do?We have a number of courses and programs we can recommend that will help prepare you for the program, depending on the areas you need to address. For example:

Intro to Machine LearningIntro to Data ScienceIntro to Programming Nanodegree programData Analyst Nanodegree programArtificial Intelligence Programming with Python Nanodegree programDeep Learning Nanodegree programMachine Learning Engineer Nanodegree program
How is this Nanodegree program structured?The Natural Language Processing Nanodegreeprogram is comprised of content and curriculum to support three (3) projects. Once you subscribe to a Nanodegree program, you will have access to the content and services for the length of time specified by your subscription. We estimate that students can complete the program in three (3) months working 10 hours per week.
Each project will be reviewed by the Udacity reviewer network. Feedback will be provided and if you do not pass the project, you will be asked to resubmit the project until it passes.
How long is this Nanodegree program?Access to this Nanodegree program runs for the length of time specified in your subscription plan. See the Terms of Use and FAQ for other policies around the terms of access to our Nanodegree programs.
Can I get a refund?Please see the Udacity Nanodegree program FAQs found here for policies on enrollment in our programs.
I have graduated from the Natural language Processing Nanodegree program but I want to keep learning. Where should I go from here?If you would like to explore other applications for convolutional and recurrent neural networks, and have an interest in computer vision, then consider enrolling in the Computer Vision Nanodegree program. If you are looking for additional advanced topics in AI, the Robotics Engineer and Self-Driving Car Engineer Nanodegree programs could be ideal for you. And regardless of your future career destination, you'll find that the Artificial Intelligence Nanodegree program is full of valuable content that will serve you well in almost any AI role.
What software and versions will I need in this program?You will need a computer running a 64-bit operating system (most modern Windows, OS X, and Linux versions will work) with at least 8GB of RAM, along with administrator account permissions sufficient to install programs including Anaconda with Python 3.5 and supporting packages. Your network should allow secure connections to remote hosts (like SSH). We will provide you with instructions to install the required software packages. Udacity does not provide any hardware or software.

        Natural language processing has come a long way since its foundations were laid in the 1940s and 50s (for an introduction see, e.g., Jurafsky and Martin (2008): Speech and Language Processing, Pearson Prentice Hall). This CRAN task view collects relevant R packages that support computational linguists in conducting analysis of speech and language on a variety of levels - setting focus on words, syntax, semantics, and pragmatics.
      

        In recent years, we have elaborated a framework to be used in
      packages dealing with the processing of written material: the package
tm.
      Extension packages in this area are highly recommended to interface with tm's basic routines
      and useRs are cordially invited to join in the discussion on further developments of this
      framework package. To get into natural language processing, the
        
          cRunch service
        
        and
        
          tutorials
        
        may be helpful.
      

tm
          provides a comprehensive text mining framework for R. The
          
            Journal of Statistical Software
          
          article
          
            Text Mining Infrastructure in R
          
          gives a detailed overview and presents techniques for count-based analysis methods, text clustering, text classification and string kernels.
        

tm.plugin.dc
          allows for distributing corpora across storage devices (local files or Hadoop Distributed File System).
        

tm.plugin.mail
          helps with importing mail messages from archive files such as used in Thunderbird (mbox, eml).
        

tm.plugin.factiva,
tm.plugin.lexisnexis,
tm.plugin.europresse
          allow importing press and Web corpora from (respectively) Dow Jones Factiva, LexisNexis, and Europresse.
        

tm.plugin.webmining
          allow importing news feeds in XML (RSS, ATOM) and JSON formats. Currently, the following feeds are implemented: Google Blog Search, Google Finance, Google News, NYTimes Article Search, Reuters News Feed, Yahoo Finance, and Yahoo Inplay.
        

RcmdrPlugin.temis
          is an Rcommander plug-in providing an integrated solution to perform a series of text mining tasks such as importing and cleaning a corpus, and analyses like terms and documents counts, vocabulary tables, terms co-occurrences and documents similarity measures, time series analysis, correspondence analysis and hierarchical clustering.
        

openNLP
          provides an R interface to
          
            OpenNLP
          , a collection of natural language processing tools including a sentence detector, tokenizer, pos-tagger, shallow and full syntactic parser, and named-entity detector, using the Maxent Java package for training and using maximum entropy models.
        

          Trained models for English and Spanish to be used with
openNLP
          are available from
          
            http://datacube.wu.ac.at/
          
          as packages openNLPmodels.en and openNLPmodels.es, respectively.
        

RWeka
          is a interface to
          
            Weka
          
          which is a collection of machine learning algorithms for data mining tasks written in Java. Especially useful in the context of natural language processing is its functionality for tokenization and stemming.
        

tidytext
          provides means for text mining for word processing and sentiment analysis using dplyr, ggplot2, and other tidy tools.
        

monkeylearn
          provides a wrapper interface to machine learning services on Monkeylearn for text analysis, i.e., classification and extraction.
        

udpipe
          provides language-independant tokenization, part of speech tagging, lemmatization, dependency parsing, and training of treebank-based annotation models.
        

          R's base package already provides a rich set of character manipulation routines. See
help.search(keyword = "character", package = "base")
          for more information on these capabilities.
        

RKEA
          provides an R interface to
          
            KEA
          
          (Version 5.0). KEA (for Keyphrase Extraction Algorithm) allows for extracting keyphrases from text documents. It can be either used for free indexing or for indexing with a controlled vocabulary.
        

gsubfn
          can be used for certain parsing tasks such as extracting words from strings by content rather than by delimiters.
demo("gsubfn-gries")
          shows an example of this in a natural language processing context.
        

textreuse
          provides a set of tools for measuring similarity among documents and helps with detecting passages which have been reused. The package implements shingled n-gram, skip n-gram, and other tokenizers; similarity/dissimilarity functions; pairwise comparisons; minhash and locality sensitive hashing algorithms; and a version of the Smith-Waterman local alignment algorithm suitable for natural language.
        

boilerpipeR
          helps with the extraction and sanitizing of text content from HTML files: removal of ads, sidebars, and headers using the boilerpipe Java library.
        

tau
          contains basic string manipulation and analysis routines needed in text processing such as dealing with character encoding, language, pattern counting, and tokenization.
        

SnowballC
          provides exactly the same API as Rstem, but uses a slightly different design of the C libstemmer library from the Snowball project. It also supports two more languages.
        

stringi
          provides R language wrappers to the International Components for Unicode (ICU) library and allows for: conversion of text encodings, string searching and collation in any locale, Unicode normalization of text, handling texts with mixed reading direction (e.g., left to right and right to left), and text boundary analysis (for tokenizing on different aggregation levels or to identify suitable line wrapping locations).
        

stringdist
          implements an approximate string matching version of R's native 'match' function. It can calculate various string distances based on edits (Damerau-Levenshtein, Hamming, Levenshtein, optimal string alignment), qgrams (q-gram, cosine, jaccard distance) or heuristic metrics (Jaro, Jaro-Winkler). An implementation of soundex is provided as well. Distances can be computed between character vectors while taking proper care of encoding or between integer vectors representing generic sequences.
        

Rstem
          (available from Omegahat) is an alternative interface to a C version of Porter's word stemming algorithm.
        

KoNLP
          provides a collection of conversion routines (e.g. Hangul to Jamos), stemming, and part of speech tagging through interfacing with the Lucene's HanNanum analyzer. In version 0.0-8.0, the documentation is sparse and still needs some help.
        

koRpus
          is a diverse collection of functions for automatic language detection, hyphenation, several indices of lexical diversity (e.g., type token ratio, HD-D/vocd-D, MTLD) and readability (e.g., Flesch, SMOG, LIX, Dale-Chall). See the
          
            web page
          
          for more information.
        

alineR
          helps calculate the phonetic distance between words (the 'ALINE' distance). The score is based on phonetic featuers represented with the Unicode-compliant International Phonetic Alphabet (IPA). Parameterized features weights are used to determine the optimal alignment and functions are provided to estimate optimum values using a genetic algorithm and supervised learning.
        

ore
          provides an alternative to R's built-in functionality for handling regular expressions, based on the Onigmo Regular Expression Library. Offers first-class compiled regex objects, partial matching and function-based substitutions, amongst other features. A benchmark comparing results for ore functions with stringi and the R base implementation is available
regex-performance.
        

languageR
          provides data sets and functions exemplifying statistical methods, and some facilitatory utility functions used in the book by R. H. Baayen: "Analyzing Linguistic Data: a Practical Introduction to Statistics Using R", Cambridge University Press, 2008.
        

zipfR
          offers some statistical models for word frequency distributions. The utilities include functions for loading, manipulating and visualizing word frequency data and vocabulary growth curves. The package also implements several statistical models for the distribution of word frequencies in a population. (The name of this library derives from the most famous word frequency distribution, Zipf's law.)
        

wordcloud
          provides a visualisation similar to the famous wordle ones: it horizontally and vertically distributes features in a pleasing visualisation with the font size scaled by frequency.
        

hunspell
          is a stemmer and spell-checker library designed for languages with rich morphology and complex word compounding or character encoding. The package can check and analyze individual words as well as search for incorrect words within a text, latex or (R package) manual document.
        

tesseract
          is an OCR engine with unicode (UTF-8) support that can recognize over 100 languages out of the box.
        

mscsweblm4r
          provides an interface to the Microsoft Cognitive Services Web Language Model API and can be used to calculate the probability for a sequence of words to appear together, the conditional probability that a specific word will follow an existing sequence of words, get the list of words (completions) most likely to follow a given sequence of words, and insert spaces into a string of words adjoined together without any spaces (hashtags, URLs, etc.).
        

mscstexta4r
          provides an interface to the Microsoft Cognitive Services Text Analytics API and can be used to perform sentiment analysis, topic detection, language detection, and key phrase extraction.
        

tokenizers
          helps split text into tokens, supporting shingled n-grams, skip n-grams, words, word stems, sentences, paragraphs, characters, lines, and regular expressions.
        

lsa
          provides routines for performing a latent semantic analysis with R. The basic idea of latent semantic analysis (LSA) is,  that text do have a higher order (=latent semantic) structure which, however, is obscured by word usage (e.g. through the use of synonyms  or polysemy). By using conceptual indices that are derived statistically via a truncated singular value decomposition (a two-mode  factor analysis) over a given document-term matrix, this variability problem can be overcome. The article
          
            Investigating Unstructured Texts with Latent Semantic Analysis
          
          gives a detailed overview and demonstrates the use of the package with examples from the are of technology-enhanced learning.
        

topicmodels
          provides an interface to the C code for Latent Dirichlet Allocation (LDA) models and Correlated Topics Models (CTM) by David M. Blei and co-authors and the C++ code for fitting LDA models using Gibbs sampling by Xuan-Hieu Phan and co-authors.
        

stm
          (Structural Topic Model) implements a topic model derivate that can include document-level meta-data. The package also includes tools for model selection, visualization, and estimation of topic-covariate regressions.
        

kernlab
          allows to create and compute with string kernels, like full string, spectrum, or bounded range string kernels. It can directly use the document format used by
tm
          as input.
        

movMF
          provides another clustering alternative (approximations are fitted with von Mises-Fisher distributions of the unit length vectors).
        

textrank
          is an extension of the PageRank and allows to summarize text by calculating how sentences are related to one another.
        

text2vec
          provides tools for text vectorization, topic modeling (LDA, LSA), word embeddings (GloVe), and similarities.
        

rel
          implements a variety of performance metrics for assessing prediction quality beyond precision and recall, including point estimates with confidence intervals for Bennett et al.'s S, Cohen's kappa, Conger's kappa, Fleiss' kappa, Gwet's AC, intraclass correlation coefficients, Krippendorff's alpha, Scott's pi, the standard error of measurement, and weighted kappa.
        

gutenbergr
          allows downloading and processing public domain works in the Project Gutenberg collection. Includes metadata for all Project Gutenberg works, so that they can be searched and retrieved.
        
This article is part of an on-going series on NLP: Part 1, Part 2, Part 3. You can also read a reader-translated version of this article in 普通话.
Giant update: I've written a new book based on these articles! It not only expands and updates all my articles, but it has tons of brand new content and lots of hands-on coding projects. Check it out now!
Computers are great at working with structured data like spreadsheets and database tables. But us humans usually communicate in words, not in tables. That's unfortunate for computers.
A lot of information in the world is unstructured — raw text in English or another human language. How can we get a computer to understand unstructured text and extract data from it?
Natural Language Processing, or NLP, is the sub-field of AI that is focused on enabling computers to understand and process human languages. Let's check out how NLP works and learn how to write programs that can extract information out of raw text using Python!
Note: If you don't care how NLP works and just want to cut and paste some code, skip way down to the section called “Coding the NLP Pipeline in Python”.
As long as computers have been around, programmers have been trying to write programs that understand languages like English. The reason is pretty obvious — humans have been writing things down for thousands of years and it would be really helpful if a computer could read and understand all that data.
Computers can't yet truly understand English in the way that humans do — but they can already do a lot! In certain limited areas, what you can do with NLP already seems like magic. You might be able to save a lot of time by applying NLP techniques to your own projects.
And even better, the latest advances in NLP are easily accessible through open source Python libraries like spaCy, textacy, and neuralcoref. What you can do with just a few lines of python is amazing.
The process of reading and understanding English is very complex — and that's not even considering that English doesn't follow logical and consistent rules. For example, what does this news headline mean?
Are the regulators questioning a business owner about burning coal illegally? Or are the regulators literally cooking the business owner? As you can see, parsing English with a computer is going to be complicated.
Doing anything complicated in machine learning usually means building a pipeline. The idea is to break up your problem into very small pieces and then use machine learning to solve each smaller piece separately. Then by chaining together several machine learning models that feed into each other, you can do very complicated things.
And that's exactly the strategy we are going to use for NLP. We'll break down the process of understanding English into small chunks and see how each one works.
This paragraph contains several useful facts. It would be great if a computer could read this text and understand that London is a city, London is located in England, London was settled by Romans and so on. But to get there, we have to first teach our computer the most basic concepts of written language and then move up from there.
The first step in the pipeline is to break the text apart into separate sentences. That gives us this:
“Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia.”
We can assume that each sentence in English is a separate thought or idea. It will be a lot easier to write a program to understand a single sentence than to understand a whole paragraph.
Coding a Sentence Segmentation model can be as simple as splitting apart sentences whenever you see a punctuation mark. But modern NLP pipelines often use more complex techniques that work even when a document isn't formatted cleanly.
Now that we've split our document into sentences, we can process them one at a time. Let's start with the first sentence from our document:
The next step in our pipeline is to break this sentence into separate words or tokens. This is called tokenization. This is the result:
Tokenization is easy to do in English. We'll just split apart words whenever there's a space between them. And we'll also treat punctuation marks as separate tokens since punctuation also has meaning.
Next, we'll look at each token and try to guess its part of speech — whether it is a noun, a verb, an adjective and so on. Knowing the role of each word in the sentence will help us start to figure out what the sentence is talking about.
We can do this by feeding each word (and some extra words around it for context) into a pre-trained part-of-speech classification model:
The part-of-speech model was originally trained by feeding it millions of English sentences with each word's part of speech already tagged and having it learn to replicate that behavior.
Keep in mind that the model is completely based on statistics — it doesn't actually understand what the words mean in the same way that humans do. It just knows how to guess a part of speech based on similar sentences and words it has seen before.
With this information, we can already start to glean some very basic meaning. For example, we can see that the nouns in the sentence include “London” and “capital”, so the sentence is probably talking about London.
In English (and most languages), words appear in different forms. Look at these two sentences:
Both sentences talk about the noun pony, but they are using different inflections. When working with text in a computer, it is helpful to know the base form of each word so that you know that both sentences are talking about the same concept. Otherwise the strings “pony” and “ponies” look like two totally different words to a computer.
In NLP, we call finding this process lemmatization — figuring out the most basic form or lemma of each word in the sentence.
The same thing applies to verbs. We can also lemmatize verbs by finding their root, unconjugated form. So “I had two ponies” becomes “I [have] two [pony].”
Lemmatization is typically done by having a look-up table of the lemma forms of words based on their part of speech and possibly having some custom rules to handle words that you've never seen before.
Here's what our sentence looks like after lemmatization adds in the root form of our verb:
Next, we want to consider the importance of a each word in the sentence. English has a lot of filler words that appear very frequently like “and”, “the”, and “a”. When doing statistics on text, these words introduce a lot of noise since they appear way more frequently than other words. Some NLP pipelines will flag them as stop words —that is, words that you might want to filter out before doing any statistical analysis.
Stop words are usually identified by just by checking a hardcoded list of known stop words. But there's no standard list of stop words that is appropriate for all applications. The list of words to ignore can vary depending on your application.
For example if you are building a rock band search engine, you want to make sure you don't ignore the word “The”. Because not only does the word “The” appear in a lot of band names, there's a famous 1980's rock band called The The!
The next step is to figure out how all the words in our sentence relate to each other. This is called dependency parsing.
The goal is to build a tree that assigns a single parent word to each word in the sentence. The root of the tree will be the main verb in the sentence. Here's what the beginning of the parse tree will look like for our sentence:
But we can go one step further. In addition to identifying the parent word of each word, we can also predict the type of relationship that exists between those two words:
This parse tree shows us that the subject of the sentence is the noun “London” and it has a “be” relationship with “capital”. We finally know something useful — London is a capital! And if we followed the complete parse tree for the sentence (beyond what is shown), we would even found out that London is the capital of the United Kingdom.
Just like how we predicted parts of speech earlier using a machine learning model, dependency parsing also works by feeding words into a machine learning model and outputting a result. But parsing word dependencies is particularly complex task and would require an entire article to explain in any detail. If you are curious how it works, a great place to start reading is Matthew Honnibal's excellent article “Parsing English in 500 Lines of Python”.
But despite a note from the author in 2015 saying that this approach is now standard, it's actually out of date and not even used by the author anymore. In 2016, Google released a new dependency parser called Parsey McParseface which outperformed previous benchmarks using a new deep learning approach which quickly spread throughout the industry. Then a year later, they released an even newer model called ParseySaurus which improved things further. In other words, parsing techniques are still an active area of research and constantly changing and improving.
It's also important to remember that many English sentences are ambiguous and just really hard to parse. In those cases, the model will make a guess based on what parsed version of the sentence seems most likely but it's not perfect and sometimes the model will be embarrassingly wrong. But over time our NLP models will continue to get better at parsing text in a sensible way.
Want to try out dependency parsing on your own sentence? There's a great interactive demo from the spaCy team here.
So far, we've treated every word in our sentence as a separate entity. But sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing.
Whether or not we do this step depends on our end goal. But it's often a quick and easy way to simplify the sentence if we don't need extra detail about which words are adjectives and instead care more about extracting complete ideas.
Now that we've done all that hard work, we can finally move beyond grade-school grammar and start actually extracting ideas.
Some of these nouns present real things in the world. For example, “London”, “England” and “United Kingdom” represent physical places on a map. It would be nice to be able to detect that! With that information, we could automatically extract a list of real-world places mentioned in a document using NLP.
The goal of Named Entity Recognition, or NER, is to detect and label these nouns with the real-world concepts that they represent. Here's what our sentence looks like after running each token through our NER tagging model:
But NER systems aren't just doing a simple dictionary lookup. Instead, they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents. A good NER system can tell the difference between “Brooklyn Decker” the person and the place “Brooklyn” using context clues.
Here are just some of the kinds of objects that a typical NER system can tag:
NER has tons of uses since it makes it so easy to grab structured data out of text. It's one of the easiest ways to quickly get value out of an NLP pipeline.
Want to try out Named Entity Recognition yourself? There's another great interactive demo from spaCy here.
At this point, we already have a useful representation of our sentence. We know the parts of speech for each word, how the words relate to each other and which words are talking about named entities.
However, we still have one big problem. English is full of pronouns — words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. Humans can keep track of what these words represent based on context. But our NLP model doesn't know what pronouns mean because it only examines one sentence at a time.
If we parse this with our NLP pipeline, we'll know that “it” was founded by Romans. But it's a lot more useful to know that “London” was founded by Romans.
As a human reading this sentence, you can easily figure out that “it” means “London”. The goal of coreference resolution is to figure out this same mapping by tracking pronouns across sentences. We want to figure out all the words that are referring to the same entity.
With coreference information combined with the parse tree and named entity information, we should be able to extract a lot of information out of this document!
Coreference resolution is one of the most difficult steps in our pipeline to implement. It's even more difficult than sentence parsing. Recent advances in deep learning have resulted in new approaches that are more accurate, but it isn't perfect yet. If you want to learn more about how it works, start here.
Want to play with co-reference resolution? Check out this great co-reference resolution demo from Hugging Face.
Note: Before we continue, it's worth mentioning that these are the steps in a typical NLP pipeline, but you will skip steps or re-order steps depending on what you want to do and how your NLP library is implemented. For example, some libraries like spaCy do sentence segmentation much later in the pipeline using the results of the dependency parse.
So how do we code this pipeline? Thanks to amazing python libraries like spaCy, it's already done! The steps are all coded and ready for you to use.
Then the code to run an NLP pipeline on a piece of text looks like this:
If you run that, you'll get a list of named entities and entity types detected in our document:
Notice that it makes a mistake on “Londinium” and thinks it is the name of a person instead of a place. This is probably because there was nothing in the training data set similar to that and it made a best guess. Named Entity Detection often requires a little bit of model fine tuning if you are parsing text that has unique or specialized terms like this.
Let's take the idea of detecting entities and twist it around to build a data scrubber. Let's say you are trying to comply with the new GDPR privacy regulations and you've discovered that you have thousands of documents with personally identifiable information in them like people's names. You've been given the task of removing any and all names from your documents.
Going through thousands of documents and trying to redact all the names by hand could take years. But with NLP, it's a breeze. Here's a simple scrubber that removes all the names it detects:
What you can do with spaCy right out of the box is pretty amazing. But you can also use the parsed output from spaCy as the input to more complex data extraction algorithms. There's a python library called textacy that implements several common data extraction algorithms on top of spaCy. It's a great starting point.
One of the algorithms it implements is called Semi-structured Statement Extraction. We can use it to search the parse tree for simple statements where the subject is “London” and the verb is a form of “be”. That should help us find facts about London.
Maybe that's not too impressive. But if you run that same code on the entire London wikipedia article text instead of just three sentences, you'll get this more impressive result:
Now things are getting interesting! That's a pretty impressive amount of information we've collected automatically.
For extra credit, try installing the neuralcoref library and adding Coreference Resolution to your pipeline. That will get you a few more facts since it will catch sentences that talk about “it” instead of mentioning “London” directly.
By looking through the spaCy docs and textacy docs, you'll see lots of examples of the ways you can work with parsed text. What we've seen so far is just a tiny sample.
Here's another practical example: Imagine that you were building a website that let's the user view information for every city in the world using the information we extracted in the last example.
If you had a search feature on the website, it might be nice to autocomplete common search queries like Google does:
But to do this, we need a list of possible completions to suggest to the user. We can use NLP to quickly generate this data.
This is just a tiny taste of what you can do with NLP. In future posts, we'll talk about other applications of NLP like Text Classification and how systems like Amazon Alexa parse questions.
But until then, install spaCy and start playing around! Or if you aren't a Python user and end up using a different NLP library, the ideas should all work roughly the same way.
This article is part of an on-going series on NLP. You can continue on to Part 2.
If you liked this article, consider signing up for my Machine Learning is Fun! newsletter:
You can also follow me on Twitter at @ageitgey, email me directly or find me on linkedin. I'd love to hear from you if I can help you or your team with machine learning.
Natural language processing (NLP) is the ability of a computer program to understand human language as it is spoken. NLP is a component of artificial intelligence (AI).
The development of NLP applications is challenging because computers traditionally require humans to "speak" to them in a programming language that is precise, unambiguous and highly structured, or through a limited number of clearly enunciated voice commands. Human speech, however, is not always precise -- it is often ambiguous and the linguistic structure can depend on many complex variables, including slang, regional dialects and social context.
Syntax and semantic analysis are two main techniques used with natural language processing. Syntax is the arrangement of words in a sentence to make grammatical sense. NLP uses syntax to assess meaning from a language based on grammatical rules. Syntax techniques used include parsing (grammatical analysis for a sentence), word segmentation (which divides a large piece of text to units), sentence breaking (which places sentence boundaries in large texts), morphological segmentation (which divides words into groups) and stemming (which divides words with inflection in them to root forms).
Semantics involves the use and meaning behind words. NLP applies algorithms to understand the meaning and structure of sentences. Techniques that NLP uses with semantics include word sense disambiguation (which derives meaning of a word based on context), named entity recognition (which determines words that can be categorized into groups), and natural language generation (which will use a database to determine semantics behind words).
Current approaches to NLP are based on deep learning, a type of AI that examines and uses patterns in data to improve a program's understanding. Deep learning models require massive amounts of labeled data to train on and identify relevant correlations, and assembling this kind of big data set is one of the main hurdles to NLP currently.
Earlier approaches to NLP involved a more rules-based approach, where simpler machine learning algorithms were told what words and phrases to look for in text and given specific responses when those phrases appeared. But deep learning is a more flexible, intuitive approach in which algorithms learn to identify speakers' intent from many examples, almost like how a child would learn human language.
Three tools used commonly for NLP include NLTK, Gensim, and Intel NLP Architect. NTLK, Natural Language Toolkit, is an open source python modules with data sets and tutorials. Gensim is a Python library for topic modeling and document indexing. Intel NLP Architect is also another Python library for deep learning topologies and techniques.
Research being done on natural language processing revolves around search, especially enterprise search. This involves allowing users to query data sets in the form of a question that they might pose to another person. The machine interprets the important elements of the human language sentence, such as those that might correspond to specific features in a data set, and returns an answer.
NLP can be used to interpret free text and make it analyzable. There is a tremendous amount of information stored in free text files, like patients' medical records, for example. Before deep learning-based NLP models, this information was inaccessible to computer-assisted analysis and could not be analyzed in any systematic way. But NLP allows analysts to sift through massive troves of free text to find relevant information in the files.
Sentiment analysis is another primary use case for NLP. Using sentiment analysis, data scientists can assess comments on social media to see how their business's brand is performing, for example, or review notes from customer service teams to identify areas where people want the business to perform better.
Google and other search engines base their machine translation technology on NLP deep learning models. This allows algorithms to read text on a webpage, interpret its meaning and translate it to another language.
The advantage of natural language processing can be seen when considering the following two statements: "Cloud computing insurance should be part of every service level agreement" and "A good SLA ensures an easier night's sleep -- even in the cloud." If you use natural language processing for search, the program will recognize that cloud computing is an entity, that cloud is an abbreviated form of cloud computing and that SLA is an industry acronym for service level agreement.
These are the types of vague elements that frequently appear in human language and that machine learning algorithms have historically been bad at interpreting. Now, with improvements in deep learning and artificial intelligence, algorithms can effectively interpret them.
This has implications for the types of data that can be analyzed. More and more information is being created online every day, and a lot of it is natural human language. Until recently, businesses have been unable to analyze this data. But advances in NLP make it possible to analyze and learn from a greater range of data sources.
NLP has not yet been wholly perfected. For example, semantic analysis can still be a challenge for NLP. Other difficulties include the fact that abstract use of language is typically tricky for programs to understand. For instance, NLP does not pick up sarcasm easily. These topics usually require the understanding of the words being used and the context in which the way they are being used. As another example, a sentence can change meaning depending on which word the speaker puts stress on. NLP is also challenged by the fact that language, and the way people use it, is continually changing. 






Margaret Rouse asks:
How does your enterprise plan to use natural language processing?



Join the Discussion





AtScale updates its data warehouse virtualization platform
AtScale 2019.1 comes with many new capabilities, including extended database support, increased security and advanced analytical ...



IBM Db2 update aims to simplify use with AI
IBM's new Db2 release adds a host of AI-powered enhancements, including a range of automated error reporting capabilities and ...



Vendor unveils Snowflake Data Exchange, Google Cloud integration
Snowflake Computing's data exchange marketplace aims to make data easily shared between users and providers, enable users to ...



IT skills shortage leads to AWS certification demand
AWS certifications help engineers advance their careers and help businesses find qualified candidates. Learn why demand for ...



6 AWS Config managed rules for secure resource configurations
Introduce these 6 managed rules for AWS Config to boost your organization's cloud security and maintain compliance across ...



NASCAR revs up video archive with AWS AI
NASCAR will convert and optimize its 18 PB library of race footage with AWS AI and other services to serve broadcast partners and...



Digital transformation technologies power CX makeovers
Businesses use many tools to effect digital transformation. Learn how organizations use these tools to improve CX and digital ...



Consider nonfunctional requirements for content services
If developers do not build security, auditing, reporting and accessibility into content services from the very beginning, they ...



Box workflow automation software, Box Relay, overhauled
Content management software vendor Box makes over Box Relay, integrating the workflow automation system more fully into the Box ...



How to connect to a pluggable database in Oracle Multitenant
Once you've created pluggable databases in an Oracle Multitenant system, the next step is to connect applications to them so they...



Creating an Oracle Multitenant container database and PDBs
Database administrator Brian Peasland outlines how to create the container and pluggable databases that are at the heart of the ...



Oracle cloud adoption and hybrid cloud adoption pros and cons
More businesses are adopting cloud systems or thinking about it. But cloud and hybrid cloud setups have pluses and minuses that ...



Demand-driven replenishment added to SAP IBP
New demand-driven planning methodology in SAP Integrated Business Planning helps organizations create better supply chain ...



Qualtrics XM platform helps identify the 'why'
Zig Serafin, president at Qualtrics, discusses the experience economy and how Qualtrics XM applications with SAP's operational ...



SAP public cloud strategy leans heavily on big-name partners
Analyst Holger Mueller says SAP is smart to stop trying to compete as a cloud infrastructure provider so it can leave that job to...



Key features to create a SQL Server audit trail in databases
SQL Server offers a set of built-in auditing tools that can help make the process of tracking logins and other database ...



Check SQL Server Query Store performance impact before using
Many IT teams hesitate to use SQL Server Query Store due to performance concerns. Consultant Andy Warren offers tips on how to ...



SQL Server vs. MySQL: Learn the differences
MySQL and Microsoft SQL Server relational databases have their pros and cons. Weigh the differences between SQL Server and MySQL ...


                Access to this page has been denied because we believe you are using automation tools to browse the
                website.
            

                Please make sure that Javascript and cookies are enabled on your browser and that you are not blocking
                them from loading.
            
Everything we express (either verbally or in written) carries huge amounts of information. The topic we choose, our tone, our selection of words, everything adds some type of information that can be interpreted and value extracted from it. In theory, we can understand and even predict human behaviour using that information.
But there is a problem: one person may generate hundreds or thousands of words in a declaration, each sentence with its corresponding complexity. If you want to scale and analyze several hundreds, thousands or millions of people or declarations in a given geography, then the situation is unmanageable.
Data generated from conversations, declarations or even tweets are examples of unstructured data. Unstructured data doesn't fit neatly into the traditional row and column structure of relational databases, and represent the vast majority of data available in the actual world. It is messy and hard to manipulate. Nevertheless, thanks to the advances in disciplines like machine learning a big revolution is going on regarding this topic. Nowadays it is no longer about trying to interpret a text or speech based on its keywords (the old fashioned mechanical way), but about understanding the meaning behind those words (the cognitive way). This way it is possible to detect figures of speech like irony, or even perform sentiment analysis.
Natural Language Processing or NLP is a field of Artificial Intelligence that gives the machines the ability to read, understand and derive meaning from human languages.
It is a discipline that focuses on the interaction between data science and human language, and is scaling to lots of industries. Today NLP is booming thanks to the huge improvements in the access to data and the increase in computational power, which are allowing practitioners to achieve meaningful results in areas like healthcare, media, finance and human resources, among others.
In simple terms, NLP represents the automatic handling of natural human language like speech or text, and although the concept itself is fascinating, the real value behind this technology comes from the use cases.
NLP can help you with lots of tasks and the fields of application just seem to increase on a daily basis. Let's mention some examples:
NLP enables the recognition and prediction of diseases based on electronic health records and patient's own speech. This capability is being explored in health conditions that go from cardiovascular diseases to depression and even schizophrenia. For example, Amazon Comprehend Medical is a service that uses NLP to extract disease conditions, medications and treatment outcomes from patient notes, clinical trial reports and other electronic health records.
Organizations can determine what customers are saying about a service or product by identifying and extracting information in sources like social media. This sentiment analysiscan provide a lot of information about customers choices and their decision drivers.
An inventor at IBM developed a cognitive assistantthat works like a personalized search engine by learning all about you and then remind you of a name, a song, or anything you can't remember the moment you need it to.
Companies like Yahoo and Google filter and classify your emails with NLP by analyzing text in emails that flow through their servers and stopping spambefore they even enter your inbox.
To help identifying fake news, the NLP Group at MITdeveloped a new system to determine if a source is accurate or politically biased, detecting if a news source can be trusted or not.
Amazon's Alexa and Apple's Siri are examples of intelligent voice driven interfacesthat use NLP to respond to vocal prompts and do everything like find a particular shop, tell us the weather forecast, suggest the best route to the office or turn on the lights at home.
Having an insight into what is happening and what people are talking about can be very valuable to financial traders. NLP is being used to track news, reports, comments about possible mergers between companies, everything can be then incorporated into a trading algorithm to generate massive profits. Remember: buy the rumor, sell the news.
NLP is also being used in both the search and selection phases of talent recruitment, identifying the skills of potential hires and also spotting prospects before they become active on the job market.
Powered by IBM Watson NLP technology, LegalMationdeveloped a platform to automate routine litigation tasks and help legal teams save time, drive down costs and shift strategic focus.
NLP is particularly booming in the healthcare industry. This technology is improving care delivery, disease diagnosis and bringing costs down while healthcare organizations are going through a growing adoption of electronic health records. The fact that clinical documentation can be improved means that patients can be better understood and benefited through better healthcare. The goal should be to optimize their experience, and several organizations are already working on this.
Number of publications containing the sentence “natural language processing” in PubMed in the period 1978–2018. As of 2018, PubMed comprised more than 29 million citations for biomedical literature
Companies like Winterlight Labs are making huge improvements in the treatment of Alzheimer's disease by monitoring cognitive impairment through speech and they can also support clinical trials and studies for a wide range of central nervous system disorders. Following a similar approach, Stanford University developed Woebot, a chatbot therapist with the aim of helping people with anxiety and other disorders.
But serious controversy is around the subject. A couple of years ago Microsoft demonstrated that by analyzing large samples of search engine queries, they could identify internet users who were suffering from pancreatic cancer even before they have received a diagnosis of the disease. How would users react to such diagnosis? And what would happen if you were tested as a false positive? (meaning that you can be diagnosed with the disease even though you don't have it). This recalls the case of Google Flu Trends which in 2009 was announced as being able to predict influenza but later on vanished due to its low accuracy and inability to meet its projected rates.
NLP may be the key to an effective clinical support in the future, but there are still many challenges to face in the short term.
The main drawbacks we face these days with NLP relate to the fact that language is very tricky. The process of understanding and manipulating language is extremely complex, and for this reason it is common to use different techniques to handle different challenges before binding everything together. Programming languages like Python or R are highly used to perform these techniques, but before diving into code lines (that will be the topic of a different article), it's important to understand the concepts beneath them. Let's summarize and explain some of the most frequently used algorithms in NLP when defining the vocabulary of terms:
Is a commonly used model that allows you to count all words in a piece of text. Basically it creates an occurrence matrix for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier.
To bring a short example I took the first sentence of the song “Across the Universe” from The Beatles:
This approach may reflect several downsides like the absence of semantic meaning and context, and the facts that stop words (like “the” or “a”) add noise to the analysis and some words are not weighted accordingly (“universe” weights less than the word “they”).
To solve this problem, one approach is to rescale the frequency of words by how often they appear in all texts (not just the one we are analyzing) so that the scores for frequent words like “the”, that are also frequent across other texts, get penalized. This approach to scoring is called “Term Frequency — Inverse Document Frequency” (TFIDF), and improves the bag of words by weights. Through TFIDF frequent terms in the text are “rewarded” (like the word “they” in our example), but they also get “punished” if those terms are frequent in other texts we include in the algorithm too. On the contrary, this method highlights and “rewards” unique or rare terms considering all texts. Nevertheless, this approach still has no context nor semantics.
Is the process of segmenting running text into sentences and words. In essence, it's the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation. Following our example, the result of tokenization would be:
Pretty simple, right? Well, although it may seem quite basic in this case and also in languages like English that separate words by a blank space (called segmented languages) not all languages behave the same, and if you think about it, blank spaces alone are not sufficient enough even for English to perform proper tokenizations. Splitting on blank spaces may break up what should be considered as one token, as in the case of certain names (e.g. San Francisco or New York) or borrowed foreign phrases (e.g. laissez faire).
Tokenization can remove punctuation too, easing the path to a proper word segmentation but also triggering possible complications. In the case of periods that follow abbreviation (e.g. dr.), the period following that abbreviation should be considered as part of the same token and not be removed.
The tokenization process can be particularly problematic when dealing with biomedical text domains which contain lots of hyphens, parentheses, and other punctuation marks.
Includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.
Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time.
There is no universal list of stop words. These can be pre-selected or built from scratch. A potential approach is to begin by adopting pre-defined stop words and add words to the list later on. Nevertheless it seems that the general trend over the past time has been to go from the use of large standard stop word lists to the use of no lists at all.
The thing is stop words removal can wipe out relevant information and modify the context in a given sentence. For example, if we are performing a sentiment analysis we might throw our algorithm off track if we remove a stop word like “not”. Under these conditions, you might select a minimal stop word list and add additional terms depending on your specific objective.
Refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word).
Affixes that are attached at the beginning of the word are called prefixes (e.g. “astro” in the word “astrobiology”) and the ones attached at the end of the word are called suffixes (e.g. “ful” in the word “helpful”).
The problem is that affixes can create or expand new forms of the same word (called inflectional affixes), or even create new words themselves (called derivational affixes). In English, prefixes are always derivational (the affix creates a new word as in the example of the prefix “eco” in the word “ecosystem”), but suffixes can be derivational (the affix creates a new word as in the example of the suffix “ist” in the word “guitarist”) or inflectional (the affix creates a new form of word as in the example of the suffix “er” in the word “faster”).
A possible approach is to consider a list of common affixes and rules (Python and R languages have different libraries containing affixes and methods) and perform stemming based on them, but of course this approach presents limitations. Since stemmers use algorithmics approaches, the result of the stemming process may not be an actual word or even change the word (and sentence) meaning. To offset this effect you can edit those predefined methods by adding or removing affixes and rules, but you must consider that you might be improving the performance in one area while producing a degradation in another one. Always look at the whole picture and test your model's performance.
So if stemming has serious limitations, why do we use it? First of all, it can be used to correct spelling errors from the tokens. Stemmers are simple to use and run very fast (they perform simple operations on a string), and if speed and performance are important in the NLP model, then stemming is certainly the way to go. Remember, we use it with the objective of improving our performance, not as a grammar exercise.
Has the objective of reducing a word to its base form and grouping together different forms of the same word. For example, verbs in past tense are changed into present (e.g. “went” is changed to “go”) and synonyms are unified (e.g. “best” is changed to “good”), hence standardizing words with similar meaning to their root. Although it seems closely related to the stemming process, lemmatization uses a different approach to reach the root forms of words.
Lemmatization resolves words to their dictionary form (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.
For example, the words “running”, “runs” and “ran” are all forms of the word “run”, so “run” is the lemma of all the previous words.
Lemmatization also takes into consideration the context of the word in order to solve other problems like disambiguation, which means it can discriminate between identical words that have different meanings depending on the specific context. Think about words like “bat” (which can correspond to the animal or to the metal/wooden club used in baseball) or “bank” (corresponding to the financial institution or to the land alongside a body of water). By providing a part-of-speech parameter to a word ( whether it is a noun, a verb, and so on) it's possible to define a role for that word in the sentence and remove disambiguation.
As you might already pictured, lemmatization is a much more resource-intensive task than performing a stemming process. At the same time, since it requires more knowledge about the language structure than a stemming approach, it demands more computational power than setting up or adapting a stemming algorithm.
Is as a method for uncovering hidden structures in sets of texts or documents. In essence it clusters texts to discover latent topics based on their contents, processing individual words and assigning them values based on their distribution. This technique is based on the assumptions that each document consists of a mixture of topics and that each topic consists of a set of words, which means that if we can spot these hidden topics we can unlock the meaning of our texts.
From the universe of topic modelling techniques, Latent Dirichlet Allocation (LDA) is probably the most commonly used. This relatively new algorithm (invented less than 20 years ago) works as an unsupervised learning method that discovers different topics underlying a collection of documents. In unsupervised learning methods like this one, there is no output variable to guide the learning process and data is explored by algorithms to find patterns. To be more specific, LDA finds groups of related words by:
Assigning each word to a random topic, where the user defines the number of topics it wishes to uncover. You don't define the topics themselves (you define just the number of topics) and the algorithm will map all documents to the topics in a way that words in each document are mostly captured by those imaginary topics.
The algorithm goes through each word iteratively and reassigns the word to a topic taking into considerations the probability that the word belongs to a topic, and the probability that the document will be generated by a topic. These probabilities are calculated multiple times, until the convergence of the algorithm.
Unlike other clustering algorithms like K-means that perform hard clustering (where topics are disjointed), LDA assigns each document to a mixture of topics, which means that each document can be described by one or more topics (e.g. Document 1 is described by 70% of topic A, 20% of topic B and 10% of topic C) and reflect more realistic results.
Topic modeling is extremely useful for classifying texts, building recommender systems (e.g. to recommend you books based on your past readings) or even detecting trends in online publications.
At the moment NLP is battling to detect nuances in language meaning, whether due to lack of context, spelling errors or dialectal differences.
On March 2016 Microsoft launched Tay, an Artificial Intelligence (AI) chatbot released on Twitter as a NLP experiment. The idea was that as more users conversed with Tay, the smarter it would get. Well, the result was that after 16 hours Tay had to be removed due to its racist and abusive comments:
Microsoft learnt from its own experience and some months later released Zo, its second generation English-language chatbot that won't be caught making the same mistakes as its predecessor. Zo uses a combination of innovative approaches to recognize and generate conversation, and other companies are exploring with bots that can remember details specific to an individual conversation.
Although the future looks extremely challenging and full of threats for NLP, the discipline is developing at a very fast pace (probably like never before) and we are likely to reach a level of advancement in the coming years that will make complex applications look possible.
Bio: Diego Lopez Yse is an experienced professional with a solid international background acquired in different industries (biotechnology, software, consultancy, government, agriculture).
A Complete Exploratory Data Analysis and Visualization for Text Data: Combine Visualization and NLP to Generate Insights
 A Step-by-Step Guide to Transitioning your Career to Data Science  Part 1
 7 Steps to Mastering SQL for Data Science  2019 Edition
 How (not) to use Machine Learning for time series forecasting: Avoiding the pitfalls
 The 3 Biggest Mistakes on Learning Data Science
 7 Steps to Mastering Intermediate Machine Learning with Python  2019 Edition
 The Third Wave Data Scientist
 The most desired skill in data science

 7 Steps to Mastering SQL for Data Science — 2019 Edition
 Python leads the 11 top Data Science, Machine Learning platforms: Trends and Analysis
 Jupyter Notebooks: Data Science Reporting
 A Step-by-Step Guide to Transitioning your Career to Data Science  Part 1
 7 Steps to Mastering Intermediate Machine Learning with Python — 2019 Edition
 The Data Fabric for Machine Learning – Part 1
 Machine Learning in Agriculture: Applications and Techniques

 Top May Stories: A Step-by-Step Guide to Transitioning ... 3 Main Approaches to Machine Learning Models The Data Fabric for Machine Learning Part 1-b – Deep ... Proxi: Data Scientist [Berkeley, CA] If you're a developer transitioning into data science... First Speakers Announced For Data Driven Government Thi...
 First Speakers Announced For Data Driven Government This Fall What you need to know: The Modern Open-Source Data Science/Mac... 5 Ways to Deal with the Lack of Data in Machine Learning Top Stories, Jun 3-9: 7 Steps to Mastering Intermediate Machin... Choosing an Error Function Digible: Data Engineer [Denver, CO] The Infinity Stones of Data Science A Step-by-Step Guide to Transitioning your Career to Data Scie... Math for Programmers. Monash University: Research Fellows, Statistical Learning, Tim... Top 10 Statistics Mistakes Made by Data Scientists Random Forests vs Neural Networks: Which is Better, and When? Using the 'What-If Tool' to investigate Machine Learning m... PyViz: Simplifying the Data Visualisation Process in Python Jupyter Notebooks: Data Science Reporting Top KDnuggets Tweets, May 29 – June 4:  Difference between #... Math for Machine Learning. NLP and Computer Vision Integrated Mongo DB Basics Apple: Data Scientist [Austin, TX]
